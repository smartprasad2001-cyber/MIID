#!/usr/bin/env python3
"""
Clean Variation Generator
Generates name, DOB, and address variations - NO VALIDATION, NO SCORING.
Just generates output.

USAGE:
    python variation_generator_clean.py example_synapse.json
"""

import re
import random
import json
import os
import sys
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional

# Import requests for Nominatim API queries
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("⚠️  Warning: requests not available. Real address generation will be disabled.")

# Import name_variations.py directly
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from name_variations import generate_name_variations

# Import jellyfish for tiered similarity generation
try:
    import jellyfish
    JELLYFISH_AVAILABLE = True
except ImportError:
    JELLYFISH_AVAILABLE = False

# Import unidecode for transliteration
try:
    from unidecode import unidecode
    UNIDECODE_AVAILABLE = True
except ImportError:
    UNIDECODE_AVAILABLE = False
    print("⚠️  Warning: unidecode not available. Non-Latin scripts may not work well.")

# Import geonamescache for getting real city names
try:
    import geonamescache
    GEONAMESCACHE_AVAILABLE = True
    # Global cache for geonames data
    _geonames_cache = None
    _cities_cache = None
    _countries_cache = None
    
    def get_geonames_data():
        """Get cached geonames data, loading it only once."""
        global _geonames_cache, _cities_cache, _countries_cache
        if _geonames_cache is None:
            _geonames_cache = geonamescache.GeonamesCache()
            _cities_cache = _geonames_cache.get_cities()
            _countries_cache = _geonames_cache.get_countries()
        return _cities_cache, _countries_cache
    
    def get_cities_for_country(country_name: str) -> List[str]:
        """Get a list of real city names for a given country."""
        if not country_name or not GEONAMESCACHE_AVAILABLE:
            return []
        
        try:
            cities, countries = get_geonames_data()
            country_name_lower = country_name.lower().strip()
            
            # Find country code
            country_code = None
            for code, data in countries.items():
                if data.get('name', '').lower().strip() == country_name_lower:
                    country_code = code
                    break
            
            if not country_code:
                return []
            
            # Get cities for this country
            country_cities = []
            for city_id, city_data in cities.items():
                if city_data.get("countrycode", "") == country_code:
                    city_name = city_data.get("name", "")
                    if city_name and len(city_name) >= 3:  # Filter very short names
                        country_cities.append(city_name)
            
            return country_cities
        except Exception as e:
            return []
            
except ImportError:
    GEONAMESCACHE_AVAILABLE = False
    _geonames_cache = None
    
    def get_cities_for_country(country_name: str) -> List[str]:
        """Fallback when geonamescache is not available."""
        return []

# Minimal IdentitySynapse class
class IdentitySynapse:
    def __init__(self, identity, query_template, timeout=120.0):
        self.identity = identity
        self.query_template = query_template
        self.timeout = timeout

# ============================================================================
# Parse Query Template
# ============================================================================

def parse_query_template(query_template: str) -> Dict:
    """Extract requirements from query template"""
    requirements = {
        'variation_count': 15,
        'rule_percentage': 0,
        'rules': [],
        'phonetic_similarity': {},
        'orthographic_similarity': {},
        'uav_seed_name': None  # Phase 3: UAV seed name
    }
    
    # Extract variation count
    count_match = re.search(r'Generate\s+(\d+)\s+variations', query_template, re.I)
    if count_match:
        requirements['variation_count'] = int(count_match.group(1))
    
    # Extract rule percentage - look for patterns like "X% of", "approximately X%", "include X%"
    rule_pct_patterns = [
        r'approximately\s+(\d+)%\s+of',  # "Approximately 24% of"
        r'also\s+include\s+(\d+)%\s+of', # "also include 44% of"
        r'(\d+)%\s+of\s+the\s+total',     # "24% of the total"
        r'(\d+)%\s+of\s+variations',      # "24% of variations"
        r'include\s+(\d+)%',              # "include 24%"
        r'(\d+)%\s+should\s+follow'       # "24% should follow"
    ]
    for pattern in rule_pct_patterns:
        rule_pct_match = re.search(pattern, query_template, re.I)
        if rule_pct_match:
            pct = rule_pct_match.group(1)
            requirements['rule_percentage'] = int(pct) / 100
            break
    
    # Extract rules - check various phrasings
    # Character replacement
    if 'replace spaces with special characters' in query_template.lower() or 'replace spaces with random special characters' in query_template.lower():
        requirements['rules'].append('replace_spaces_with_special_characters')
    if 'replace double letters' in query_template.lower() or 'replace double letters with single letter' in query_template.lower():
        requirements['rules'].append('replace_double_letters')
    if 'replace random vowels' in query_template.lower() or 'replace vowels with different vowels' in query_template.lower():
        requirements['rules'].append('replace_random_vowels')
    if 'replace random consonants' in query_template.lower() or 'replace consonants with different consonants' in query_template.lower():
        requirements['rules'].append('replace_random_consonants')
    
    # Character swapping
    if 'swap adjacent consonants' in query_template.lower():
        requirements['rules'].append('swap_adjacent_consonants')
    if 'swap adjacent syllables' in query_template.lower():
        requirements['rules'].append('swap_adjacent_syllables')
    if 'swap random letter' in query_template.lower() or 'swap random adjacent letters' in query_template.lower():
        requirements['rules'].append('swap_random_letter')
    
    # Character removal
    if 'delete a random letter' in query_template.lower() or 'delete random letter' in query_template.lower():
        requirements['rules'].append('delete_random_letter')
    if 'remove random vowel' in query_template.lower() or 'remove a random vowel' in query_template.lower():
        requirements['rules'].append('remove_random_vowel')
    if 'remove random consonant' in query_template.lower() or 'remove a random consonant' in query_template.lower():
        requirements['rules'].append('remove_random_consonant')
    if 'remove all spaces' in query_template.lower() or 'remove spaces' in query_template.lower():
        requirements['rules'].append('remove_all_spaces')
    
    # Character insertion
    if 'duplicate a random letter' in query_template.lower() or 'duplicate random letter' in query_template.lower():
        requirements['rules'].append('duplicate_random_letter')
    if 'insert random letter' in query_template.lower() or 'insert a random letter' in query_template.lower():
        requirements['rules'].append('insert_random_letter')
    if 'add a title prefix' in query_template.lower() or 'title prefix' in query_template.lower() or 'add title prefix' in query_template.lower():
        requirements['rules'].append('add_title_prefix')
    if 'add a title suffix' in query_template.lower() or 'title suffix' in query_template.lower() or 'add title suffix' in query_template.lower():
        requirements['rules'].append('add_title_suffix')
    
    # Name formatting
    if 'use first name initial' in query_template.lower() or 'first name initial with last name' in query_template.lower():
        requirements['rules'].append('initial_only_first_name')
    if 'convert name to initials' in query_template.lower() or 'shorten name to initials' in query_template.lower():
        requirements['rules'].append('shorten_to_initials')
    if 'abbreviate name parts' in query_template.lower() or 'abbreviate' in query_template.lower() or 'shorten name to abbreviations' in query_template.lower():
        requirements['rules'].append('abbreviate_name_parts')
    
    # Structure change
    if 'reorder name parts' in query_template.lower() or 'reorder parts' in query_template.lower() or 'name parts permutations' in query_template.lower():
        requirements['rules'].append('reorder_name_parts')
    
    # Extract phonetic similarity distribution (Light/Medium/Far percentages)
    # First try VALIDATION HINTS section (more reliable format)
    phonetic_match = re.search(r'\[VALIDATION HINTS\].*?Phonetic similarity:\s*([^.;]+)', query_template, re.I | re.DOTALL)
    if phonetic_match:
        hints_text = phonetic_match.group(1)
        # Extract percentages from hints: "10% Light, 50% Medium, 40% Far"
        hints_match = re.search(r'(\d+)%\s+Light.*?(\d+)%\s+Medium.*?(\d+)%\s+Far', hints_text, re.I)
        if hints_match:
            light_pct = int(hints_match.group(1)) / 100.0
            medium_pct = int(hints_match.group(2)) / 100.0
            far_pct = int(hints_match.group(3)) / 100.0
            requirements['phonetic_similarity'] = {
                'Light': light_pct,
                'Medium': medium_pct,
                'Far': far_pct
            }
    
    # If not found in VALIDATION HINTS, try other patterns
    if 'phonetic_similarity' not in requirements or not requirements['phonetic_similarity']:
        phonetic_match = re.search(r'phonetic similarity.*?distribution.*?(\d+)%\s+Light.*?(\d+)%\s+Medium.*?(\d+)%\s+Far', query_template, re.I | re.DOTALL)
        if not phonetic_match:
            # Try alternative patterns
            phonetic_match = re.search(r'phonetic similarity.*?(\d+)%\s+Light.*?(\d+)%\s+Medium.*?(\d+)%\s+Far', query_template, re.I | re.DOTALL)
        if phonetic_match:
            light_pct = int(phonetic_match.group(1)) / 100.0
            medium_pct = int(phonetic_match.group(2)) / 100.0
            far_pct = int(phonetic_match.group(3)) / 100.0
            requirements['phonetic_similarity'] = {
                'Light': light_pct,
                'Medium': medium_pct,
                'Far': far_pct
            }
        else:
            # Fallback: check for simpler patterns
            if 'phonetic similarity' in query_template.lower():
                # Default to Medium if no specific distribution found
                requirements['phonetic_similarity'] = {'Medium': 1.0}
    
    # Extract orthographic similarity distribution (Light/Medium/Far percentages)
    # First try VALIDATION HINTS section (more reliable format)
    orthographic_match = re.search(r'\[VALIDATION HINTS\].*?Orthographic similarity:\s*([^.;]+)', query_template, re.I | re.DOTALL)
    if orthographic_match:
        hints_text = orthographic_match.group(1)
        # Extract percentages from hints: "70% Light, 30% Medium" or "70% Light, 30% Medium, 0% Far"
        hints_match = re.search(r'(\d+)%\s+Light.*?(\d+)%\s+Medium(?:.*?(\d+)%\s+Far)?', hints_text, re.I)
        if hints_match:
            light_pct = int(hints_match.group(1)) / 100.0
            medium_pct = int(hints_match.group(2)) / 100.0
            far_pct = int(hints_match.group(3)) / 100.0 if hints_match.lastindex >= 3 and hints_match.group(3) else 0.0
            requirements['orthographic_similarity'] = {
                'Light': light_pct,
                'Medium': medium_pct
            }
            if far_pct > 0:
                requirements['orthographic_similarity']['Far'] = far_pct
    
    # If not found in VALIDATION HINTS, try other patterns
    if 'orthographic_similarity' not in requirements or not requirements['orthographic_similarity']:
        orthographic_match = re.search(r'orthographic similarity.*?distribution.*?(\d+)%\s+Light.*?(\d+)%\s+Medium', query_template, re.I | re.DOTALL)
        if not orthographic_match:
            # Try alternative patterns (may include Far)
            orthographic_match = re.search(r'orthographic similarity.*?(\d+)%\s+Light.*?(\d+)%\s+Medium(?:.*?(\d+)%\s+Far)?', query_template, re.I | re.DOTALL)
        if orthographic_match:
            light_pct = int(orthographic_match.group(1)) / 100.0
            medium_pct = int(orthographic_match.group(2)) / 100.0
            far_pct = int(orthographic_match.group(3)) / 100.0 if orthographic_match.lastindex >= 3 and orthographic_match.group(3) else 0.0
            requirements['orthographic_similarity'] = {
                'Light': light_pct,
                'Medium': medium_pct
            }
            if far_pct > 0:
                requirements['orthographic_similarity']['Far'] = far_pct
        else:
            # Fallback: check for simpler patterns
            if 'orthographic similarity' in query_template.lower():
                # Default to Medium if no specific distribution found
                requirements['orthographic_similarity'] = {'Medium': 1.0}
    
    # Extract UAV seed name from Phase 3 requirements
    uav_match = re.search(r'For the seed "([^"]+)" ONLY', query_template, re.I)
    if uav_match:
        requirements['uav_seed_name'] = uav_match.group(1)
    
    return requirements

# ============================================================================
# Rule Application Functions
# ============================================================================

def apply_replace_spaces_with_special_chars(name: str) -> str:
    """Replace spaces with special characters"""
    if ' ' not in name:
        return name
    special_chars = ['_', '-', '@', '.']
    return name.replace(' ', random.choice(special_chars))

def apply_delete_random_letter(name: str) -> str:
    """Delete a random letter"""
    if len(name) <= 1:
        return name
    idx = random.randint(0, len(name) - 1)
    return name[:idx] + name[idx+1:]

def apply_replace_double_letters(name: str) -> str:
    """Replace double letters with single letter"""
    name_lower = name.lower()
    for i in range(len(name_lower) - 1):
        if name_lower[i] == name_lower[i+1] and name[i].isalpha():
            return name[:i+1] + name[i+2:]
    return name

def apply_swap_adjacent_consonants(name: str) -> str:
    """Swap adjacent consonants"""
    vowels = "aeiou"
    name_lower = name.lower()
    for i in range(len(name_lower) - 1):
        if (name_lower[i].isalpha() and name_lower[i] not in vowels and
            name_lower[i+1].isalpha() and name_lower[i+1] not in vowels and
            name_lower[i] != name_lower[i+1]):
            return name[:i] + name[i+1] + name[i] + name[i+2:]
    return name

def apply_swap_adjacent_syllables(name: str) -> str:
    """Swap adjacent syllables (simplified: swap name parts)"""
    parts = name.split()
    if len(parts) >= 2:
        # Swap first and last name
        return " ".join([parts[-1]] + parts[1:-1] + [parts[0]])
    elif len(parts) == 1:
        # For single word, try to split in middle and swap
        word = parts[0]
        mid = len(word) // 2
        if mid > 0:
            return word[mid:] + word[:mid]
    return name

def apply_add_title_suffix(name: str) -> str:
    """Add a title suffix (Jr., PhD, etc.)"""
    suffixes = ['Jr.', 'Sr.', 'PhD', 'MD', 'III', 'II', 'Esq.']
    return name + " " + random.choice(suffixes)

def apply_abbreviate_name_parts(name: str) -> str:
    """Abbreviate name parts (e.g., "John" -> "J.")"""
    parts = name.split()
    if len(parts) >= 2:
        # Abbreviate first name
        parts[0] = parts[0][0] + "." if len(parts[0]) > 0 else parts[0]
    elif len(parts) == 1 and len(parts[0]) > 1:
        # If single word, abbreviate first letter
        parts[0] = parts[0][0] + "."
    return " ".join(parts)

def apply_replace_random_vowels(name: str) -> str:
    """Replace random vowels with different vowels"""
    vowels = {'a': ['e', 'i', 'o', 'u'], 'e': ['a', 'i', 'o', 'u'], 'i': ['a', 'e', 'o', 'u'],
              'o': ['a', 'e', 'i', 'u'], 'u': ['a', 'e', 'i', 'o'],
              'A': ['E', 'I', 'O', 'U'], 'E': ['A', 'I', 'O', 'U'], 'I': ['A', 'E', 'O', 'U'],
              'O': ['A', 'E', 'I', 'U'], 'U': ['A', 'E', 'I', 'O']}
    
    result = list(name)
    vowel_indices = [i for i, char in enumerate(name) if char.lower() in 'aeiou']
    
    if vowel_indices:
        # Replace 1-2 random vowels
        num_replacements = min(random.randint(1, 2), len(vowel_indices))
        indices_to_replace = random.sample(vowel_indices, num_replacements)
        
        for idx in indices_to_replace:
            char = name[idx]
            if char in vowels:
                result[idx] = random.choice(vowels[char])
    
    return ''.join(result)

def apply_remove_all_spaces(name: str) -> str:
    """Remove all spaces from name"""
    return name.replace(' ', '')

def apply_reorder_name_parts(name: str) -> str:
    """Reorder name parts (swap, reverse, etc.)"""
    parts = name.split()
    if len(parts) >= 2:
        # Different reordering strategies
        strategy = random.choice(['swap_first_last', 'reverse_all', 'random_shuffle'])
        
        if strategy == 'swap_first_last':
            # Swap first and last
            return " ".join([parts[-1]] + parts[1:-1] + [parts[0]])
        elif strategy == 'reverse_all':
            # Reverse all parts
            return " ".join(reversed(parts))
        else:  # random_shuffle
            # Shuffle all parts
            shuffled = parts.copy()
            random.shuffle(shuffled)
            return " ".join(shuffled)
    elif len(parts) == 1:
        # For single word, reverse it
        return parts[0][::-1]
    return name

def apply_replace_random_consonants(name: str) -> str:
    """Replace random consonants with different consonants"""
    consonants = {
        'b': ['c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'c': ['b', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'd': ['b', 'c', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'f': ['b', 'c', 'd', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'g': ['b', 'c', 'd', 'f', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'h': ['b', 'c', 'd', 'f', 'g', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'j': ['b', 'c', 'd', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'k': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'l': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'm': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'n': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'p': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'q', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'q': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r', 's', 't', 'v', 'w', 'x', 'z'],
        'r': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 's', 't', 'v', 'w', 'x', 'z'],
        's': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 't', 'v', 'w', 'x', 'z'],
        't': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 'v', 'w', 'x', 'z'],
        'v': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'w', 'x', 'z'],
        'w': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'x', 'z'],
        'x': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'z'],
        'z': ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x']
    }
    # Add uppercase versions
    for key in list(consonants.keys()):
        consonants[key.upper()] = [c.upper() for c in consonants[key]]
    
    result = list(name)
    consonant_indices = [i for i, char in enumerate(name) if char.isalpha() and char.lower() not in 'aeiou']
    
    if consonant_indices:
        # Replace 1-2 random consonants
        num_replacements = min(random.randint(1, 2), len(consonant_indices))
        indices_to_replace = random.sample(consonant_indices, num_replacements)
        
        for idx in indices_to_replace:
            char = name[idx]
            if char.lower() in consonants:
                result[idx] = random.choice(consonants[char.lower() if char.islower() else char.upper()])
    
    return ''.join(result)

def apply_swap_random_letter(name: str) -> str:
    """Swap random adjacent letters (not just consonants)"""
    if len(name) < 2:
        return name
    
    # Find all adjacent letter pairs (case-insensitive, any letters)
    swap_candidates = []
    for i in range(len(name) - 1):
        if name[i].isalpha() and name[i+1].isalpha() and name[i].lower() != name[i+1].lower():
            swap_candidates.append(i)
    
    if swap_candidates:
        idx = random.choice(swap_candidates)
        return name[:idx] + name[idx+1] + name[idx] + name[idx+2:]
    
    return name

def apply_remove_random_vowel(name: str) -> str:
    """Remove a random vowel"""
    vowels = 'aeiouAEIOU'
    vowel_indices = [i for i, char in enumerate(name) if char in vowels]
    
    if vowel_indices:
        idx = random.choice(vowel_indices)
        return name[:idx] + name[idx+1:]
    
    return name

def apply_remove_random_consonant(name: str) -> str:
    """Remove a random consonant"""
    consonant_indices = [i for i, char in enumerate(name) if char.isalpha() and char.lower() not in 'aeiou']
    
    if consonant_indices:
        idx = random.choice(consonant_indices)
        return name[:idx] + name[idx+1:]
    
    return name

def apply_duplicate_random_letter(name: str) -> str:
    """Duplicate a random letter"""
    if len(name) == 0:
        return name
    
    letter_indices = [i for i, char in enumerate(name) if char.isalpha()]
    
    if letter_indices:
        idx = random.choice(letter_indices)
        return name[:idx+1] + name[idx] + name[idx+1:]
    
    return name

def apply_insert_random_letter(name: str) -> str:
    """Insert a random letter"""
    if len(name) == 0:
        return random.choice('abcdefghijklmnopqrstuvwxyz')
    
    # Insert at random position
    idx = random.randint(0, len(name))
    random_letter = random.choice('abcdefghijklmnopqrstuvwxyz')
    
    # Preserve case context
    if idx > 0 and name[idx-1].isupper():
        random_letter = random_letter.upper()
    
    return name[:idx] + random_letter + name[idx:]

def apply_add_title_prefix(name: str) -> str:
    """Add a title prefix (Mr., Dr., etc.)"""
    prefixes = ['Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.', 'Rev.', 'Sir', 'Lady']
    return random.choice(prefixes) + " " + name

def apply_initial_only_first_name(name: str) -> str:
    """Use first name initial with last name (e.g., 'John Doe' -> 'J. Doe')"""
    parts = name.split()
    if len(parts) >= 2:
        parts[0] = parts[0][0] + "." if len(parts[0]) > 0 else parts[0]
        return " ".join(parts)
    elif len(parts) == 1 and len(parts[0]) > 1:
        return parts[0][0] + "."
    return name

def apply_shorten_to_initials(name: str) -> str:
    """Convert name to initials (e.g., 'John Doe' -> 'J. D.')"""
    parts = name.split()
    if len(parts) >= 2:
        initials = [part[0] + "." for part in parts if len(part) > 0]
        return " ".join(initials)
    elif len(parts) == 1 and len(parts[0]) > 1:
        return parts[0][0] + "."
    return name

def apply_rule_to_name(name: str, rule: str) -> str:
    """Apply a rule to a name"""
    rule_map = {
        # Character replacement
        'replace_spaces_with_special_characters': apply_replace_spaces_with_special_chars,
        'replace_double_letters': apply_replace_double_letters,
        'replace_random_vowels': apply_replace_random_vowels,
        'replace_random_consonants': apply_replace_random_consonants,
        
        # Character swapping
        'swap_adjacent_consonants': apply_swap_adjacent_consonants,
        'swap_adjacent_syllables': apply_swap_adjacent_syllables,
        'swap_random_letter': apply_swap_random_letter,
        
        # Character removal
        'delete_random_letter': apply_delete_random_letter,
        'remove_random_vowel': apply_remove_random_vowel,
        'remove_random_consonant': apply_remove_random_consonant,
        'remove_all_spaces': apply_remove_all_spaces,
        
        # Character insertion
        'duplicate_random_letter': apply_duplicate_random_letter,
        'insert_random_letter': apply_insert_random_letter,
        'add_title_prefix': apply_add_title_prefix,
        'add_title_suffix': apply_add_title_suffix,
        
        # Name formatting
        'initial_only_first_name': apply_initial_only_first_name,
        'shorten_to_initials': apply_shorten_to_initials,
        'abbreviate_name_parts': apply_abbreviate_name_parts,
        
        # Structure change
        'reorder_name_parts': apply_reorder_name_parts,
        
        # Aliases for validator rule names
        'replace_spaces_with_random_special_characters': apply_replace_spaces_with_special_chars,
        'replace_double_letters_with_single_letter': apply_replace_double_letters,
        'replace_random_vowel_with_random_vowel': apply_replace_random_vowels,
        'replace_random_consonant_with_random_consonant': apply_replace_random_consonants,
        'duplicate_random_letter_as_double_letter': apply_duplicate_random_letter,
        'add_random_leading_title': apply_add_title_prefix,
        'add_random_trailing_title': apply_add_title_suffix,
        'shorten_name_to_initials': apply_shorten_to_initials,
        'shorten_name_to_abbreviations': apply_abbreviate_name_parts,
        'name_parts_permutations': apply_reorder_name_parts,
    }
    func = rule_map.get(rule)
    return func(name) if func else name

# ============================================================================
# DOB Variations
# ============================================================================

def generate_dob_variations(dob: str, count: int = 15) -> List[str]:
    """Generate DOB variations"""
    try:
        base_date = datetime.strptime(dob, "%Y-%m-%d")
    except:
        base_date = datetime(1990, 1, 1)
    
    variations = []
    
    # ±1 day
    variations.append((base_date + timedelta(days=1)).strftime("%Y-%m-%d"))
    variations.append((base_date - timedelta(days=1)).strftime("%Y-%m-%d"))
    
    # ±3 days
    variations.append((base_date + timedelta(days=3)).strftime("%Y-%m-%d"))
    variations.append((base_date - timedelta(days=3)).strftime("%Y-%m-%d"))
    
    # ±30 days
    variations.append((base_date + timedelta(days=30)).strftime("%Y-%m-%d"))
    variations.append((base_date - timedelta(days=30)).strftime("%Y-%m-%d"))
    
    # ±90 days
    variations.append((base_date + timedelta(days=90)).strftime("%Y-%m-%d"))
    variations.append((base_date - timedelta(days=90)).strftime("%Y-%m-%d"))
    
    # ±365 days
    variations.append((base_date + timedelta(days=365)).strftime("%Y-%m-%d"))
    variations.append((base_date - timedelta(days=365)).strftime("%Y-%m-%d"))
    
    # Year+month only
    variations.append(base_date.strftime("%Y-%m"))
    
    # Fill remaining with random variations
    while len(variations) < count:
        days_offset = random.randint(-365, 365)
        new_date = base_date + timedelta(days=days_offset)
        variations.append(new_date.strftime("%Y-%m-%d"))
    
    return variations[:count]

# ============================================================================
# Address Variations
# ============================================================================

def validate_city_in_country(city_name: str, country_name: str) -> bool:
    """
    Validate that a city exists in the country using geonamescache.
    Uses the same logic as the validator's city_in_country function.
    """
    if not city_name or not country_name or not GEONAMESCACHE_AVAILABLE:
        return False
    
    try:
        cities, countries = get_geonames_data()
        city_name_lower = city_name.lower().strip()
        country_name_lower = country_name.lower().strip()
        
        # Find country code
        country_code = None
        for code, data in countries.items():
            if data.get('name', '').lower().strip() == country_name_lower:
                country_code = code
                break
        
        if not country_code:
            return False
        
        # Only check cities that are actually in the specified country
        city_words = city_name_lower.split()
        
        for city_id, city_data in cities.items():
            # Skip cities not in the target country
            if city_data.get("countrycode", "") != country_code:
                continue
                
            city_data_name = city_data.get("name", "").lower().strip()
            
            # Check exact match first (validator's logic)
            if city_data_name == city_name_lower:
                return True
            # Check first word match
            elif len(city_words) >= 2 and city_data_name.startswith(city_words[0]):
                return True
            # Check second word match
            elif len(city_words) >= 2 and city_words[1] in city_data_name:
                return True
        
        return False
    except Exception:
        return False

def normalize_country_name(country: str) -> str:
    """
    Normalize country name to match validator's COUNTRY_MAPPING.
    This ensures region matching works correctly.
    """
    # Import COUNTRY_MAPPING from validator (or duplicate the mapping)
    COUNTRY_MAPPING = {
        "korea, south": "south korea",
        "korea, north": "north korea",
        "cote d ivoire": "ivory coast",
        "côte d'ivoire": "ivory coast",
        "cote d'ivoire": "ivory coast",
        "the gambia": "gambia",
        "netherlands": "the netherlands",
        "holland": "the netherlands",
        "congo, democratic republic of the": "democratic republic of the congo",
        "democratic republic of the": "democratic republic of the congo",  # Added variant for truncated country names
        "drc": "democratic republic of the congo",
        "congo, republic of the": "republic of the congo",
        "burma": "myanmar",
        "bonaire": "bonaire, saint eustatius and saba",
        "usa": "united states",
        "us": "united states",
        "united states of america": "united states",
        "uk": "united kingdom",
        "great britain": "united kingdom",
        "britain": "united kingdom",
        "uae": "united arab emirates",
        "u.s.a.": "united states",
        "u.s.": "united states",
        "u.k.": "united kingdom",
    }
    
    country_lower = country.lower().strip()
    normalized = COUNTRY_MAPPING.get(country_lower, country_lower)
    # Return original format but with normalized value for lookup
    # Preserve original case/format but use normalized for validation
    return normalized

# Well-known cities for countries that might not be in geonamescache or when lookup fails
# Mapped from sanctioned_countries.json - all countries should have real cities here
WELL_KNOWN_CITIES = {
    # Latin script countries
    "cuba": ["Havana", "Santiago de Cuba", "Camagüey", "Holguín", "Santa Clara", "Guantánamo", "Bayamo", "Cienfuegos"],
    "venezuela": ["Caracas", "Maracaibo", "Valencia", "Barquisimeto", "Ciudad Guayana", "Mérida", "San Cristóbal", "Barinas"],
    "south sudan": ["Juba", "Malakal", "Wau", "Yei", "Bentiu", "Aweil", "Rumbek", "Torit"],
    "central african republic": ["Bangui", "Bimbo", "Berbérati", "Carnot", "Bambari", "Bouar", "Bossangoa", "Bria"],
    "democratic republic of the congo": ["Kinshasa", "Lubumbashi", "Mbuji-Mayi", "Bukavu", "Kananga", "Kisangani", "Goma", "Matadi"],
    "democratic republic of the": ["Kinshasa", "Lubumbashi", "Mbuji-Mayi", "Bukavu", "Kananga", "Kisangani", "Goma", "Matadi"],  # Variant
    "mali": ["Bamako", "Sikasso", "Mopti", "Koutiala", "Kayes", "Ségou", "Gao", "Timbuktu"],
    "nicaragua": ["Managua", "León", "Granada", "Masaya", "Matagalpa", "Chinandega", "Estelí", "Jinotega"],
    "angola": ["Luanda", "Huambo", "Lobito", "Benguela", "Kuito", "Lubango", "Malanje", "Namibe"],
    "bolivia": ["La Paz", "Santa Cruz", "Cochabamba", "Sucre", "Oruro", "Tarija", "Potosí", "Trinidad"],
    "burkina faso": ["Ouagadougou", "Bobo-Dioulasso", "Koudougou", "Ouahigouya", "Banfora", "Dédougou", "Kaya", "Tenkodogo"],
    "cameroon": ["Douala", "Yaoundé", "Garoua", "Bafoussam", "Bamenda", "Maroua", "Kribi", "Buea"],
    "ivory coast": ["Abidjan", "Bouaké", "Daloa", "Yamoussoukro", "San-Pédro", "Korhogo", "Man", "Divo"],
    "côte d'ivoire": ["Abidjan", "Bouaké", "Daloa", "Yamoussoukro", "San-Pédro", "Korhogo", "Man", "Divo"],  # Variant
    "cote d'ivoire": ["Abidjan", "Bouaké", "Daloa", "Yamoussoukro", "San-Pédro", "Korhogo", "Man", "Divo"],  # Variant
    "british virgin islands": ["Road Town", "Spanish Town", "East End", "The Valley", "Great Harbour"],
    "haiti": ["Port-au-Prince", "Carrefour", "Delmas", "Pétion-Ville", "Gonaïves", "Cap-Haïtien", "Saint-Marc", "Les Cayes"],
    "kenya": ["Nairobi", "Mombasa", "Kisumu", "Nakuru", "Eldoret", "Thika", "Malindi", "Kitale"],
    "monaco": ["Monaco", "Monte Carlo", "Fontvieille"],
    "mozambique": ["Maputo", "Matola", "Beira", "Nampula", "Chimoio", "Nacala", "Quelimane", "Tete"],
    "namibia": ["Windhoek", "Rundu", "Walvis Bay", "Oshakati", "Swakopmund", "Katima Mulilo", "Grootfontein", "Mariental"],
    "nigeria": ["Lagos", "Kano", "Ibadan", "Abuja", "Port Harcourt", "Benin City", "Kaduna", "Maiduguri"],
    "south africa": ["Johannesburg", "Cape Town", "Durban", "Pretoria", "Port Elizabeth", "Bloemfontein", "East London", "Polokwane"],
    "myanmar": ["Yangon", "Mandalay", "Naypyidaw", "Mawlamyine", "Taunggyi", "Monywa", "Sittwe", "Pathein"],
    "burma": ["Yangon", "Mandalay", "Naypyidaw", "Mawlamyine", "Taunggyi", "Monywa", "Sittwe", "Pathein"],  # Variant
    "laos": ["Vientiane", "Savannakhet", "Pakse", "Luang Prabang", "Phonsavan", "Thakhek", "Xam Neua", "Muang Xay"],
    "nepal": ["Kathmandu", "Pokhara", "Patan", "Biratnagar", "Birgunj", "Dharan", "Bharatpur", "Janakpur"],
    "vietnam": ["Ho Chi Minh City", "Hanoi", "Da Nang", "Haiphong", "Can Tho", "Hue", "Nha Trang", "Quy Nhon"],
    
    # Arabic script countries
    "iran": ["Tehran", "Mashhad", "Isfahan", "Karaj", "Shiraz", "Tabriz", "Qom", "Ahvaz"],
    "afghanistan": ["Kabul", "Kandahar", "Herat", "Mazar-i-Sharif", "Jalalabad", "Kunduz", "Ghazni", "Balkh"],
    "sudan": ["Khartoum", "Omdurman", "Port Sudan", "Kassala", "El Geneina", "Nyala", "Al-Fashir", "Kosti"],
    "iraq": ["Baghdad", "Basra", "Mosul", "Erbil", "Najaf", "Karbala", "Kirkuk", "Ramadi"],
    "lebanon": ["Beirut", "Tripoli", "Sidon", "Tyre", "Zahle", "Byblos", "Baalbek", "Jounieh"],
    "libya": ["Tripoli", "Benghazi", "Misrata", "Bayda", "Zawiya", "Ajdabiya", "Tobruk", "Sabha"],
    "somalia": ["Mogadishu", "Hargeisa", "Kismayo", "Bosaso", "Baidoa", "Beledweyne", "Galkayo", "Garowe"],
    "yemen": ["Sana'a", "Aden", "Ta'izz", "Hodeidah", "Ibb", "Dhamar", "Sayyan", "Zinjibar"],
    "algeria": ["Algiers", "Oran", "Constantine", "Annaba", "Blida", "Batna", "Djelfa", "Sétif"],
    "syria": ["Damascus", "Aleppo", "Homs", "Latakia", "Hama", "Tartus", "Deir ez-Zor", "Raqqa"],
    
    # CJK script countries
    "north korea": ["Pyongyang", "Hamhung", "Chongjin", "Nampo", "Wonsan", "Sinuiju", "Tanchon", "Kaechon"],
    
    # Cyrillic script countries
    "russia": ["Moscow", "Saint Petersburg", "Novosibirsk", "Yekaterinburg", "Kazan", "Nizhny Novgorod", "Chelyabinsk", "Samara"],
    "crimea": ["Simferopol", "Sevastopol", "Yalta", "Kerch", "Feodosia", "Evpatoria", "Bakhchisaray", "Sudak"],
    "donetsk": ["Donetsk", "Mariupol", "Makiivka", "Horlivka", "Kramatorsk", "Sloviansk", "Bakhmut", "Pokrovsk"],
    "luhansk": ["Luhansk", "Alchevsk", "Sievierodonetsk", "Lysychansk", "Stakhanov", "Krasnyi Luch", "Antratsyt", "Pervomaisk"],
    "belarus": ["Minsk", "Gomel", "Mogilev", "Vitebsk", "Grodno", "Brest", "Bobruisk", "Baranavichy"],
    "bulgaria": ["Sofia", "Plovdiv", "Varna", "Burgas", "Ruse", "Stara Zagora", "Pleven", "Sliven"],
    "ukraine": ["Kyiv", "Kharkiv", "Odesa", "Dnipro", "Donetsk", "Zaporizhzhia", "Lviv", "Kryvyi Rih"],
    
    # Additional common variations
    "republic of the congo": ["Brazzaville", "Pointe-Noire", "Dolisie", "Nkayi", "Ouesso", "Owando"],
    "the netherlands": ["Amsterdam", "Rotterdam", "The Hague", "Utrecht", "Eindhoven", "Groningen", "Tilburg", "Almere"],
    "netherlands": ["Amsterdam", "Rotterdam", "The Hague", "Utrecht", "Eindhoven", "Groningen", "Tilburg", "Almere"],
    "holland": ["Amsterdam", "Rotterdam", "The Hague", "Utrecht", "Eindhoven", "Groningen", "Tilburg", "Almere"],
    "south korea": ["Seoul", "Busan", "Incheon", "Daegu", "Daejeon", "Gwangju", "Ulsan", "Seongnam"],
    "gambia": ["Banjul", "Serekunda", "Brikama", "Bakau", "Farafenni", "Lamin", "Sukuta", "Basse Santa Su"],
    "the gambia": ["Banjul", "Serekunda", "Brikama", "Bakau", "Farafenni", "Lamin", "Sukuta", "Basse Santa Su"],
    "united arab emirates": ["Dubai", "Abu Dhabi", "Sharjah", "Al Ain", "Ajman", "Ras Al Khaimah", "Fujairah", "Umm Al Quwain"],
    "uae": ["Dubai", "Abu Dhabi", "Sharjah", "Al Ain", "Ajman", "Ras Al Khaimah", "Fujairah", "Umm Al Quwain"],
    "united kingdom": ["London", "Birmingham", "Manchester", "Glasgow", "Liverpool", "Leeds", "Edinburgh", "Sheffield"],
    "uk": ["London", "Birmingham", "Manchester", "Glasgow", "Liverpool", "Leeds", "Edinburgh", "Sheffield"],
    "great britain": ["London", "Birmingham", "Manchester", "Glasgow", "Liverpool", "Leeds", "Edinburgh", "Sheffield"],
    "britain": ["London", "Birmingham", "Manchester", "Glasgow", "Liverpool", "Leeds", "Edinburgh", "Sheffield"],
    "united states": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego"],
    "usa": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego"],
    "us": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix", "Philadelphia", "San Antonio", "San Diego"],
}

def get_fallback_cities(country_name: str) -> List[str]:
    """
    Get fallback cities for a country when geonamescache fails.
    
    Strategy:
    1. First try WELL_KNOWN_CITIES database (for sanctioned countries)
    2. If not found, try geonamescache directly (should work for most countries)
    3. If geonamescache also fails, return empty list (will use country name extraction)
    
    Returns empty list if no fallback cities available.
    """
    country_lower = country_name.lower().strip()
    normalized = normalize_country_name(country_name)
    
    # Strategy 1: Try WELL_KNOWN_CITIES database first (for sanctioned countries)
    # Try normalized name first
    if normalized in WELL_KNOWN_CITIES:
        return WELL_KNOWN_CITIES[normalized]
    
    # Try original name
    if country_lower in WELL_KNOWN_CITIES:
        return WELL_KNOWN_CITIES[country_lower]
    
    # Try partial match for long country names
    for key, cities in WELL_KNOWN_CITIES.items():
        if country_lower in key or key in country_lower:
            return cities
    
    # Strategy 2: Try geonamescache directly as fallback (for valid countries)
    # This should work for most countries from geonamescache
    if GEONAMESCACHE_AVAILABLE:
        try:
            cities, countries = get_geonames_data()
            
            # Find country code
            country_code = None
            for code, data in countries.items():
                if data.get('name', '').lower().strip() == normalized:
                    country_code = code
                    break
                if data.get('name', '').lower().strip() == country_lower:
                    country_code = code
                    break
            
            if country_code:
                # Get cities for this country
                country_cities = []
                for city_id, city_data in cities.items():
                    if city_data.get("countrycode", "") == country_code:
                        city_name = city_data.get("name", "").strip()
                        if city_name and len(city_name) > 2:  # Filter very short names
                            country_cities.append(city_name)
                
                # Return up to 10 cities (should be enough)
                if country_cities:
                    return list(set(country_cities))[:10]  # Remove duplicates and limit
        except Exception:
            # If geonamescache lookup fails, continue to next strategy
            pass
    
    # Strategy 3: Return empty list (will use country name extraction as last resort)
    return []

# ============================================================================
# Real Address Generation - Hardcoded Database of Street Names
# ============================================================================

# Load hardcoded database of real street names per country
_real_street_names_db: Dict[str, List[str]] = {}
_db_loaded = False

def _load_street_names_database():
    """Load the hardcoded database of real street names per country."""
    global _real_street_names_db, _db_loaded
    
    if _db_loaded:
        return _real_street_names_db
    
    try:
        # Try to load from JSON file first
        db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'real_street_names_db.json')
        if os.path.exists(db_path):
            with open(db_path, 'r', encoding='utf-8') as f:
                _real_street_names_db = json.load(f)
            _db_loaded = True
            return _real_street_names_db
    except Exception:
        pass
    
    # If file doesn't exist, use the inline database (defined below)
    _real_street_names_db = _INLINE_STREET_NAMES_DB
    _db_loaded = True
    return _real_street_names_db

# Inline database of real street names (fallback if JSON file not available)
# This is populated from real_street_names_db.json or generated on-demand
_INLINE_STREET_NAMES_DB: Dict[str, List[str]] = {}

def get_real_street_names_for_country(country: str) -> List[str]:
    """
    Get real street names for a specific country from the hardcoded database.
    
    Args:
        country: Country name (normalized)
        
    Returns:
        List of real street names for that country
    """
    db = _load_street_names_database()
    
    # Try exact match first
    if country in db:
        return db[country]
    
    # Try normalized country name
    normalized = normalize_country_name(country)
    if normalized in db:
        return db[normalized]
    
    # Try case-insensitive lookup
    country_lower = country.lower()
    for key, streets in db.items():
        if key.lower() == country_lower:
            return streets
    
    # Try partial match
    for key, streets in db.items():
        if country_lower in key.lower() or key.lower() in country_lower:
            return streets
    
    # Return empty list if not found (will use fallback)
    return []

def get_real_addresses_from_nominatim(city: str, country: str, limit: int = 20) -> List[str]:
    """
    Query Nominatim API for real addresses in a specific city/country.
    Results are cached per city+country to avoid repeated API calls.
    
    Args:
        city: City name
        country: Country name (normalized)
        limit: Maximum number of addresses to fetch
        
    Returns:
        List of real addresses from OSM (formatted as "number street, city, country")
    """
    if not REQUESTS_AVAILABLE:
        return []
    
    # Create cache key
    cache_key = f"{city.lower()},{country.lower()}"
    
    # Return cached results if available
    if cache_key in _real_addresses_cache:
        return _real_addresses_cache[cache_key]
    
    try:
        # Strategy: Query for various place types in the city to get street names
        # We'll accept results with place_rank >= 18 (neighborhood level or better)
        # This gives us more results while still being reasonably specific
        
        url = "https://nominatim.openstreetmap.org/search"
        headers = {
            "User-Agent": "MIID-Subnet-Miner/1.0 (https://github.com/yanezcompliance/MIID-subnet; miner@yanezcompliance.com)"
        }
        
        all_results = []
        
        # Try different query strategies
        queries = [
            f"{city}, {country}",  # Simple city, country (gets various places)
        ]
        
        for query in queries:
            params = {
                "q": query,
                "format": "json",
                "limit": limit * 5,  # Fetch many results to filter
                "addressdetails": 1,
                "extratags": 1,
                "namedetails": 1
            }
            
            try:
                response = requests.get(url, params=params, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    results = response.json()
                    if results:
                        all_results.extend(results)
                
                # Rate limiting: wait 1 second between queries
                time.sleep(1.0)
                break  # Only try first query for now
            except Exception:
                continue
        
        if not all_results:
            return []
        
        # Extract street names and format addresses
        real_addresses = []
        seen_addresses = set()
        seen_roads = set()  # Track unique road names
        
        for result in all_results:
            # Accept street-level, building-level, or neighborhood-level results
            # place_rank >= 18 includes neighborhoods, streets, and buildings
            place_rank = result.get('place_rank', 0)
            if place_rank < 18:
                continue
            
            # Extract address components
            display_name = result.get('display_name', '')
            address_details = result.get('address', {})
            
            # Try to extract street/road name from various fields
            road = (
                address_details.get('road', '') or
                address_details.get('street', '') or
                address_details.get('street_name', '') or
                address_details.get('residential', '') or
                address_details.get('pedestrian', '') or
                address_details.get('path', '')
            )
            
            # Also check result type - if it's a highway/road, use the name
            result_type = result.get('type', '')
            result_class = result.get('class', '')
            if (result_class == 'highway' or result_type in ['residential', 'primary', 'secondary', 'tertiary', 'unclassified']) and not road:
                # Use the name field if it's a road
                road = result.get('name', '')
            
            # Fallback: try to extract from display_name
            if not road and display_name:
                parts = display_name.split(',')
                if len(parts) > 0:
                    first_part = parts[0].strip()
                    # Check if first part looks like a street name (not a number, not too short)
                    if len(first_part) > 3 and not first_part.replace(' ', '').isdigit():
                        # Try to extract street name (might have number prefix)
                        street_match = re.match(r'^(\d+)\s+(.+?)$', first_part)
                        if street_match:
                            road = street_match.group(2).strip()
                        elif 'street' in first_part.lower() or 'road' in first_part.lower() or 'avenue' in first_part.lower():
                            road = first_part
            
            # If we have a road/street name, format the address
            if road and len(road) > 2 and road.lower() not in seen_roads:
                seen_roads.add(road.lower())
                
                # Extract house number if available
                house_number = address_details.get('house_number', '')
                if not house_number and display_name:
                    # Try to extract number from display_name
                    number_match = re.search(r'\b(\d+)\b', display_name.split(',')[0])
                    if number_match:
                        house_number = number_match.group(1)
                
                # Use house_number if available, otherwise generate a random number
                number = house_number if house_number else str(random.randint(1, 999))
                
                # Format address: "number street, city, country"
                formatted_addr = f"{number} {road}, {city}, {country}"
                
                # Normalize to avoid duplicates
                normalized_addr = formatted_addr.lower().strip()
                if normalized_addr not in seen_addresses:
                    real_addresses.append(formatted_addr)
                    seen_addresses.add(normalized_addr)
                    
                    if len(real_addresses) >= limit:
                        break
        
        # Cache the results (even if empty, to avoid repeated failed queries)
        _real_addresses_cache[cache_key] = real_addresses
        
        # Rate limiting: wait 1 second after API call (Nominatim policy)
        time.sleep(1.0)
        
        return real_addresses
        
    except Exception as e:
        # On error, return empty list (will fallback to generic addresses)
        print(f"⚠️  Warning: Failed to fetch real addresses from Nominatim for {city}, {country}: {str(e)}")
        return []

def generate_address_variations(address: str, count: int = 15) -> List[str]:
    """
    Generate address variations - uses real city names from geonamescache when available.
    
    CRITICAL FIX: Validates cities against geonamescache to ensure they pass
    validator's extract_city_country and city_in_country checks (Address Regain Match score).
    """
    # Extract city/country from address - preserve EXACT country name format
    parts = address.split(',')
    original_country = None
    seed_city = None
    
    if len(parts) >= 2:
        # Has comma: "City, Country" format
        seed_city = parts[0].strip()
        original_country = parts[-1].strip()  # Preserve EXACT country name format
    else:
        # No comma: validator sent just country name
        original_country = address.strip() if address.strip() else "Unknown"
    
    # Normalize country name for geonamescache lookup (validator does this too)
    normalized_country = normalize_country_name(original_country)
    
    # Get cities for this country - BUT validate they exist in geonamescache
    if seed_city and validate_city_in_country(seed_city, normalized_country):
        # If seed city is valid, use it
        city_pool = [seed_city]
    else:
        # Get all cities for this country and filter to only validated ones
        all_cities = get_cities_for_country(normalized_country)
        # Filter to only cities that pass validator's city_in_country check
        city_pool = [city for city in all_cities if validate_city_in_country(city, normalized_country)]
        
        # If no validated cities found, try fallback cities from well-known database
        if not city_pool:
            fallback_cities = get_fallback_cities(original_country)
            if fallback_cities:
                # Try to validate fallback cities against geonamescache
                validated_fallbacks = [city for city in fallback_cities if validate_city_in_country(city, normalized_country)]
                if validated_fallbacks:
                    city_pool = validated_fallbacks
                else:
                    # Use fallback cities even if not validated (better than "City")
                    city_pool = fallback_cities
            else:
                # Last resort: try to use first word of country name or a generic name
                # Extract a meaningful word from country name instead of "City"
                country_words = normalized_country.split()
                if len(country_words) > 0:
                    # Use first significant word (skip "the", "of", etc.)
                    significant_words = [w for w in country_words if w.lower() not in ["the", "of", "and", "republic", "democratic"]]
                    if significant_words:
                        fallback_name = significant_words[0].capitalize()
                        city_pool = [fallback_name]
                    else:
                        city_pool = ["City"]  # Absolute last resort
                else:
                    city_pool = ["City"]  # Absolute last resort
    
    variations = []
    used = set()
    
    # Get real street names from hardcoded database for this country
    real_street_names = get_real_street_names_for_country(normalized_country)
    
    # Determine if we should use real street names or fallback to generic
    has_real_streets = len(real_street_names) > 0
    
    if has_real_streets:
        # Use real street names from hardcoded database
        # Generate addresses: "number street, city, country"
        building_numbers = list(range(1, 999))
        
        for i in range(count):
            street = random.choice(real_street_names)
            number = random.choice(building_numbers)
            city = random.choice(city_pool)
            
            addr = f"{number} {street}, {city}, {normalized_country}"
            
            if addr not in used:
                variations.append(addr)
                used.add(addr)
            else:
                # Add apartment number if duplicate
                apt = random.randint(1, 999)
                addr = f"{number} {street}, Apt {apt}, {city}, {normalized_country}"
                variations.append(addr)
                used.add(addr)
    else:
        # Fallback to generic street names if database doesn't have this country
        street_names = ["Main St", "Oak Ave", "Park Rd", "Elm St", "First Ave", 
                        "Second St", "Broadway", "Washington Ave", "Lincoln St"]
        building_numbers = list(range(1, 999))
        
        for i in range(count):
            street = random.choice(street_names)
            number = random.choice(building_numbers)
            city = random.choice(city_pool)
            
            addr = f"{number} {street}, {city}, {normalized_country}"
            
            if addr not in used:
                variations.append(addr)
                used.add(addr)
            else:
                # Add apartment number if duplicate
                apt = random.randint(1, 999)
                addr = f"{number} {street}, Apt {apt}, {city}, {normalized_country}"
                variations.append(addr)
                used.add(addr)
    
    return variations[:count]

def generate_uav_address(address: str) -> Dict:
    """
    Generate UAV (Unknown Attack Vector) address that looks valid but might fail geocoding.
    Returns: dict with 'address', 'label', 'latitude', 'longitude'
    """
    # Extract city/country from address (same logic as generate_address_variations)
    parts = address.split(',')
    original_country = None
    seed_city = None
    
    if len(parts) >= 2:
        seed_city = parts[0].strip()
        original_country = parts[-1].strip()
    else:
        # No comma: validator sent just country name
        original_country = address.strip() if address.strip() else "Unknown"
    
    # Normalize country name for geonamescache lookup (same as generate_address_variations)
    normalized_country = normalize_country_name(original_country)
    
    # Get cities for this country - BUT validate they exist in geonamescache
    if seed_city and validate_city_in_country(seed_city, normalized_country):
        # If seed city is valid, use it
        city_pool = [seed_city]
    else:
        # Get all cities for this country and filter to only validated ones
        all_cities = get_cities_for_country(normalized_country)
        # Filter to only cities that pass validator's city_in_country check
        city_pool = [city for city in all_cities if validate_city_in_country(city, normalized_country)]
        
        # If no validated cities found, try fallback cities from well-known database
        if not city_pool:
            fallback_cities = get_fallback_cities(original_country)
            if fallback_cities:
                # Try to validate fallback cities against geonamescache
                validated_fallbacks = [city for city in fallback_cities if validate_city_in_country(city, normalized_country)]
                if validated_fallbacks:
                    city_pool = validated_fallbacks
                else:
                    # Use fallback cities even if not validated (better than "City")
                    city_pool = fallback_cities
            else:
                # Last resort: try to use first word of country name or a generic name
                # Extract a meaningful word from country name instead of "City"
                country_words = normalized_country.split()
                if len(country_words) > 0:
                    # Use first significant word (skip "the", "of", etc.)
                    significant_words = [w for w in country_words if w.lower() not in ["the", "of", "and", "republic", "democratic"]]
                    if significant_words:
                        fallback_name = significant_words[0].capitalize()
                        city_pool = [fallback_name]
                    else:
                        city_pool = ["City"]  # Absolute last resort
                else:
                    city_pool = ["City"]  # Absolute last resort
    
    # Select a random city from the pool
    city = random.choice(city_pool)
    
    # Get real street names from hardcoded database for this country
    real_street_names = get_real_street_names_for_country(normalized_country)
    
    # Generate an address with a potential issue (typo, abbreviation, etc.)
    # CRITICAL: Use normalized_country (not original_country) to match validator expectations
    num = random.randint(1, 999)
    
    if real_street_names:
        # Use a real street name as base and modify it to create a UAV (typo, abbreviation, etc.)
        street = random.choice(real_street_names)
        
        # Create UAV variations from real street name
        uav_options = [
            (f"{num} {street}tr, {city}, {normalized_country}", "Common typo (Str vs St)"),
            (f"{num} {street[:15]} Av, {city}, {normalized_country}", "Local abbreviation (Av vs Ave)"),
            (f"{num} 1st {street}, {city}, {normalized_country}", "Missing street direction"),
            (f"{num}, {city}, {normalized_country}", "Number without street name"),
            (f"{num} {street}., {city}, {normalized_country}", "Abbreviated with period"),
        ]
        uav_address, label = random.choice(uav_options)
    else:
        # Fallback to generic if no real street names available from database
        uav_options = [
            (f"{num} Main Str, {city}, {normalized_country}", "Common typo (Str vs St)"),
            (f"{num} Oak Av, {city}, {normalized_country}", "Local abbreviation (Av vs Ave)"),
            (f"{num} 1st St, {city}, {normalized_country}", "Missing street direction"),
            (f"{num}, {city}, {normalized_country}", "Number without street name"),
            (f"{num} Elm St., {city}, {normalized_country}", "Abbreviated with period"),
        ]
        uav_address, label = random.choice(uav_options)
    
    # Generate realistic coordinates based on country (approximate)
    # Comprehensive country database with geographic centers
    # These are rough approximations - in production, use geocoding API
    country_coords = {
        # North America
        "USA": (39.8283, -98.5795), "United States": (39.8283, -98.5795),
        "US": (39.8283, -98.5795), "United States of America": (39.8283, -98.5795),
        "Canada": (56.1304, -106.3468), "Mexico": (23.6345, -102.5528),
        # Central America & Caribbean
        "Haiti": (18.9712, -72.2852), "Honduras": (15.2000, -86.2419),
        "Cuba": (21.5218, -77.7812), "Jamaica": (18.1096, -77.2975),
        "Guatemala": (15.7835, -90.2308), "Belize": (17.1899, -88.4976),
        "El Salvador": (13.7942, -88.8965), "Nicaragua": (12.2650, -85.2072),
        "Costa Rica": (9.7489, -83.7534), "Panama": (8.5380, -80.7821),
        "Dominican Republic": (18.7357, -70.1627), "Puerto Rico": (18.2208, -66.5901),
        # South America
        "Brazil": (-14.2350, -51.9253), "Argentina": (-38.4161, -63.6167),
        "Colombia": (4.5709, -74.2973), "Peru": (-9.1900, -75.0152),
        "Venezuela": (6.4238, -66.5897), "Chile": (-35.6751, -71.5430),
        "Ecuador": (-1.8312, -78.1834), "Bolivia": (-16.2902, -63.5887),
        "Paraguay": (-23.4425, -58.4438), "Uruguay": (-32.5228, -55.7658),
        # Europe
        "UK": (54.7024, -3.2766), "United Kingdom": (54.7024, -3.2766),
        "Britain": (54.7024, -3.2766), "Great Britain": (54.7024, -3.2766),
        "Germany": (51.1657, 10.4515), "France": (46.2276, 2.2137),
        "Spain": (40.4637, -3.7492), "Italy": (41.8719, 12.5674),
        "Russia": (61.5240, 105.3188), "Poland": (51.9194, 19.1451),
        "Netherlands": (52.1326, 5.2913), "Belgium": (50.5039, 4.4699),
        "Greece": (39.0742, 21.8243), "Portugal": (39.3999, -8.2245),
        "Sweden": (60.1282, 18.6435), "Norway": (60.4720, 8.4689),
        "Denmark": (56.2639, 9.5018), "Finland": (61.9241, 25.7482),
        "Switzerland": (46.8182, 8.2275), "Austria": (47.5162, 14.5501),
        "Czech Republic": (49.8175, 15.4730), "Romania": (45.9432, 24.9668),
        "Hungary": (47.1625, 19.5033), "Ukraine": (48.3794, 31.1656),
        "Turkey": (38.9637, 35.2433), "Ireland": (53.4129, -8.2439),
        # Asia
        "China": (35.8617, 104.1954), "India": (20.5937, 78.9629),
        "Japan": (36.2048, 138.2529), "South Korea": (35.9078, 127.7669),
        "North Korea": (40.3399, 127.5101), "Thailand": (15.8700, 100.9925),
        "Vietnam": (14.0583, 108.2772), "Philippines": (12.8797, 121.7740),
        "Indonesia": (-0.7893, 113.9213), "Malaysia": (4.2105, 101.9758),
        "Singapore": (1.3521, 103.8198), "Bangladesh": (23.6850, 90.3563),
        "Pakistan": (30.3753, 69.3451), "Afghanistan": (33.9391, 67.7100),
        "Iran": (32.4279, 53.6880), "Iraq": (33.2232, 43.6793),
        "Saudi Arabia": (23.8859, 45.0792), "UAE": (23.4241, 53.8478),
        "United Arab Emirates": (23.4241, 53.8478), "Israel": (31.0461, 34.8516),
        "Lebanon": (33.8547, 35.8623), "Jordan": (30.5852, 36.2384),
        "Syria": (34.8021, 38.9968), "Yemen": (15.5527, 48.5164),
        # Africa
        "Egypt": (26.8206, 30.8025), "Libya": (26.3351, 17.2283),
        "Sudan": (12.8628, 30.2176), "Ethiopia": (9.1450, 38.7667),
        "Kenya": (-0.0236, 37.9062), "Nigeria": (9.0820, 8.6753),
        "South Africa": (-30.5595, 22.9375), "Ghana": (7.9465, -1.0232),
        "Morocco": (31.7917, -7.0926), "Algeria": (28.0339, 1.6596),
        "Tunisia": (33.8869, 9.5375), "Mauritius": (-20.3484, 57.5522),
        "Gabon": (-0.8037, 11.6094), "Benin": (9.3077, 2.3158),
        "Namibia": (-22.9576, 18.4904), "Papua New Guinea": (-6.3150, 143.9555),
        # Additional sanctioned countries
        "South Sudan": (6.8770, 31.3070), "Central African Republic": (6.6111, 20.9394),
        "Democratic Republic of the Congo": (-4.0383, 21.7587), "DRC": (-4.0383, 21.7587),
        "Congo, Democratic Republic of the": (-4.0383, 21.7587), "Mali": (17.5707, -3.9962),
        "Angola": (-11.2027, 17.8739), "Burkina Faso": (12.2383, -1.5616),
        "Cameroon": (7.3697, 12.3547), "Ivory Coast": (7.5400, -5.5471),
        "Cote d'Ivoire": (7.5400, -5.5471), "British Virgin Islands": (18.4207, -64.6399),
        "Monaco": (43.7384, 7.4246), "Mozambique": (-18.6657, 35.5296),
        "Myanmar": (21.9162, 95.9560), "Laos": (19.8563, 102.4955),
        "Nepal": (28.3949, 84.1240), "Somalia": (5.1521, 46.1996),
        # Additional Cyrillic sanctioned regions
        "Belarus": (53.7098, 27.9534), "Bulgaria": (42.7339, 25.4858),
        "Crimea": (45.3388, 33.5000), "Donetsk": (48.0159, 37.8029),
        "Luhansk": (48.5740, 39.3078),
        # Oceania
        "Australia": (-25.2744, 133.7751), "New Zealand": (-40.9006, 174.8860),
        # Middle East (already listed above, keeping for organization)
        "Kuwait": (29.3117, 47.4818), "Qatar": (25.3548, 51.1839),
        "Bahrain": (25.9304, 50.6378), "Oman": (21.4735, 55.9754),
    }
    
    # Normalize country name for matching (lowercase, handle common variations)
    # Use normalized_country for coordinate lookup
    country_for_coords = normalized_country.strip().lower()
    
    # Try to find country in our map (case-insensitive, partial matching)
    lat, lon = None, None
    for country_key, coords in country_coords.items():
        country_key_lower = country_key.lower()
        # Exact match or substring match (either direction)
        if (country_key_lower == country_for_coords or
            country_key_lower in country_for_coords or
            country_for_coords in country_key_lower):
            lat, lon = coords
            # Add small random offset to make it unique (within ~50km)
            lat += random.uniform(-0.5, 0.5)
            lon += random.uniform(-0.5, 0.5)
            break
    
    # Fallback: Try to get approximate coordinates for unrecognized countries
    # Use a basic heuristic based on country name patterns
    if lat is None or lon is None:
        # For unknown countries, generate coordinates in reasonable ranges
        # Most countries are between -60 and 70 latitude
        lat = random.uniform(-35, 60)
        lon = random.uniform(-180, 180)
        # Log for debugging (use original_country for display)
        if original_country:
            print(f"   ⚠️  Country '{original_country}' not found in database, using approximate coordinates")
    
    return {
        'address': uav_address,
        'label': label,
        'latitude': round(lat, 6),
        'longitude': round(lon, 6)
    }

# ============================================================================
# Non-Latin Script Detection and Variations
# ============================================================================

def detect_script(name: str) -> str:
    """Detect the script type of a name"""
    # Check for Arabic characters
    if re.search(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]', name):
        return 'arabic'
    # Check for Cyrillic characters
    if re.search(r'[\u0400-\u04FF\u0500-\u052F\u2DE0-\u2DFF\uA640-\uA69F]', name):
        return 'cyrillic'
    # Check for Chinese/Japanese/Korean characters
    if re.search(r'[\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF\uAC00-\uD7AF]', name):
        return 'cjk'
    # Check if contains non-Latin characters
    if re.search(r'[^\x00-\x7F]', name):
        return 'non-latin'
    return 'latin'

def generate_non_latin_variations(name: str, script: str, count: int) -> List[str]:
    """Generate variations for non-Latin script names"""
    variations = []
    used = set([name.lower()])
    
    # Strategy 1: Script-specific transformations (keep original script) - PRIORITIZE THESE
    parts = name.split()
    
    # For Arabic/Cyrillic: Swap similar-looking characters, add/remove spaces
    if script in ['arabic', 'cyrillic']:
        # Swap adjacent parts
        if len(parts) >= 2:
            swapped = " ".join([parts[-1]] + parts[:-1])
            if swapped.lower() not in used:
                variations.append(swapped)
                used.add(swapped.lower())
        
        # Remove spaces (merge parts)
        if len(parts) >= 2:
            merged = "".join(parts)
            if merged.lower() not in used:
                variations.append(merged)
                used.add(merged.lower())
        
        # Add space in middle of long words
        for idx, part in enumerate(parts):
            if len(part) > 4:
                mid = len(part) // 2
                spaced = part[:mid] + " " + part[mid:]
                var = " ".join(parts[:idx] + [spaced] + parts[idx+1:])
                if var.lower() not in used:
                    variations.append(var)
                    used.add(var.lower())
                    break
        
        # Reverse parts order
        if len(parts) >= 2:
            reversed_parts = " ".join(parts[::-1])
            if reversed_parts.lower() not in used:
                variations.append(reversed_parts)
                used.add(reversed_parts.lower())
    
    # For CJK: Character-level variations
    if script == 'cjk':
        # Swap characters
        if len(parts) >= 2:
            swapped = " ".join([parts[-1]] + parts[:-1])
            if swapped.lower() not in used:
                variations.append(swapped)
                used.add(swapped.lower())
    
    # Strategy 2: Transliterate and generate variations (mix with script-specific)
    transliterated_vars = []
    if UNIDECODE_AVAILABLE and len(variations) < count:
        transliterated = unidecode(name)
        if transliterated and transliterated != name:
            # Generate variations on transliterated version (limit to avoid filling all slots)
            latin_vars = generate_name_variations(transliterated, limit=max(count - len(variations), count // 2))
            # Keep transliterated variations (valid for non-Latin names)
            for var in latin_vars:
                if var.lower() not in used:
                    transliterated_vars.append(var)
                    used.add(var.lower())
    
    # Mix script-specific and transliterated variations (prioritize script-specific)
    # Add script-specific first, then interleave with transliterated
    final_variations = variations[:]  # Copy script-specific variations
    translit_idx = 0
    while len(final_variations) < count and translit_idx < len(transliterated_vars):
        final_variations.append(transliterated_vars[translit_idx])
        translit_idx += 1
    variations = final_variations
    
    # Strategy 3: Character-level transformations (work for all scripts)
    # Generate variations by removing/duplicating/inserting characters
    max_char_variations = count
    attempts = 0
    while len(variations) < count and attempts < count * 3:
        attempts += 1
        
        # Remove a character
        if len(name) > 2:
            idx = random.randint(0, len(name) - 1)
            var = name[:idx] + name[idx+1:]
            if var and var.lower() not in used:
                variations.append(var)
                used.add(var.lower())
                if len(variations) >= count:
                    break
        
        # Duplicate a character
        if len(name) > 1:
            idx = random.randint(0, len(name) - 1)
            var = name[:idx+1] + name[idx] + name[idx+1:]
            if var.lower() not in used:
                variations.append(var)
                used.add(var.lower())
                if len(variations) >= count:
                    break
        
        # Swap adjacent characters (if not already done)
        if len(name) >= 2:
            idx = random.randint(0, len(name) - 2)
            var = name[:idx] + name[idx+1] + name[idx] + name[idx+2:]
            if var.lower() not in used:
                variations.append(var)
                used.add(var.lower())
                if len(variations) >= count:
                    break
    
    # Strategy 4: Add more transliterated variations if we still need more
    if UNIDECODE_AVAILABLE and len(variations) < count:
        transliterated = unidecode(name)
        if transliterated and transliterated != name:
            # Get more transliterated variations
            remaining = count - len(variations)
            more_latin_vars = generate_name_variations(transliterated, limit=remaining * 3)
            for var in more_latin_vars:
                if len(variations) >= count:
                    break
                if var.lower() not in used:
                    variations.append(var)
                    used.add(var.lower())
    
    # Strategy 5: If still not enough, create simple variations by modifying parts
    if len(variations) < count:
        remaining = count - len(variations)
        for i in range(remaining * 2):
            if len(variations) >= count:
                break
            parts = name.split()
            if len(parts) >= 2:
                # Try different part combinations
                if i % 3 == 0:
                    var = " ".join(parts[::-1])
                elif i % 3 == 1:
                    var = "".join(parts)
                else:
                    var = parts[-1] + " " + " ".join(parts[:-1])
            elif len(parts) == 1 and len(parts[0]) > 1:
                # For single word, try removing characters from different positions
                word = parts[0]
                idx = (i * 3) % (len(word) - 1) + 1
                var = word[:idx] + word[idx+1:]
            else:
                continue
            
            if var and var.lower() not in used:
                variations.append(var)
                used.add(var.lower())
    
    return variations[:count]

# ============================================================================
# Tiered Similarity Generation (Using Jellyfish)
# ============================================================================

def calculate_phonetic_similarity_score(original: str, variation: str) -> float:
    """
    Calculate phonetic similarity score using same logic as validator.
    Uses randomized subset of Soundex, Metaphone, NYSIIS.
    Returns: similarity score between 0.0 and 1.0
    """
    if not JELLYFISH_AVAILABLE:
        return 0.5  # Fallback medium similarity
    
    try:
        # Use same logic as validator - randomized subset of algorithms
        algorithms = {
            "soundex": lambda x, y: jellyfish.soundex(x) == jellyfish.soundex(y),
            "metaphone": lambda x, y: jellyfish.metaphone(x) == jellyfish.metaphone(y),
            "nysiis": lambda x, y: jellyfish.nysiis(x) == jellyfish.nysiis(y),
        }
        
        # Deterministically seed based on original name (same as validator)
        random.seed(hash(original) % 10000)
        selected_algorithms = random.sample(list(algorithms.keys()), k=min(3, len(algorithms)))
        
        # Generate random weights that sum to 1.0 (same as validator)
        weights = [random.random() for _ in selected_algorithms]
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        # Calculate weighted phonetic score
        phonetic_score = sum(
            (1.0 if algorithms[algo](original, variation) else 0.0) * weight
            for algo, weight in zip(selected_algorithms, normalized_weights)
        )
        
        return float(phonetic_score)
    except Exception:
        return 0.5  # Fallback

def calculate_orthographic_similarity_score(original: str, variation: str) -> float:
    """
    Calculate orthographic similarity score using same logic as validator.
    Uses Levenshtein distance normalized to 0-1.
    Returns: similarity score between 0.0 and 1.0
    """
    if not JELLYFISH_AVAILABLE:
        return 0.5  # Fallback
    
    try:
        # Use same logic as validator - Levenshtein distance
        distance = jellyfish.levenshtein_distance(original.lower(), variation.lower())
        max_len = max(len(original), len(variation))
        
        if max_len == 0:
            return 1.0
        
        # Calculate similarity score (0-1), same as validator
        similarity = 1.0 - (distance / max_len)
        return float(similarity)
    except Exception:
        return 0.5  # Fallback

def get_phonetic_tier_from_score(score: float) -> str:
    """
    Categorize phonetic similarity score into Light/Medium/Far tier.
    Uses validator's exact boundaries: Light (0.80-1.00), Medium (0.60-0.79), Far (0.30-0.59)
    """
    if score >= 0.80:
        return 'Light'
    elif score >= 0.60:
        return 'Medium'
    elif score >= 0.30:
        return 'Far'
    else:
        return 'Far'  # Very low similarity

def get_orthographic_tier_from_score(score: float) -> str:
    """
    Categorize orthographic similarity score into Light/Medium/Far tier.
    Uses validator's exact boundaries: Light (0.70-1.00), Medium (0.50-0.69), Far (0.20-0.49)
    """
    if score >= 0.70:
        return 'Light'
    elif score >= 0.50:
        return 'Medium'
    elif score >= 0.20:
        return 'Far'
    else:
        return 'Far'  # Very low similarity

def get_levenshtein_tier(original: str, candidate: str) -> str:
    """
    Determine orthographic similarity tier using actual similarity score calculation.
    Returns: 'Light', 'Medium', or 'Far'
    """
    score = calculate_orthographic_similarity_score(original, candidate)
    return get_orthographic_tier_from_score(score)

def get_metaphone_match_score(original: str, candidate: str) -> str:
    """
    Determine phonetic similarity tier using actual similarity score calculation.
    Returns: 'Light', 'Medium', or 'Far'
    """
    score = calculate_phonetic_similarity_score(original, candidate)
    return get_phonetic_tier_from_score(score)

def generate_tiered_name_variations(
    original_name: str,
    non_rule_count: int,
    phonetic_similarity: Dict[str, float] = None,
    orthographic_similarity: Dict[str, float] = None
) -> List[str]:
    """
    Generate name variations targeting specific Light/Medium/Far distributions.
    
    Uses jellyfish (Double Metaphone + Levenshtein) to categorize variations
    and select them to match the target distribution.
    """
    # Generate a large candidate pool using name_variations.py
    # Request 10x more candidates to ensure we have enough in each tier
    candidate_pool = generate_name_variations(original_name, limit=non_rule_count * 10)
    
    # Remove original name from pool
    candidate_pool = [c for c in candidate_pool if c.lower() != original_name.lower()]
    
    if not candidate_pool:
        # Fallback: generate simple variations
        return generate_name_variations(original_name, limit=non_rule_count)
    
    # If no similarity requirements specified, use default (all Medium)
    if not phonetic_similarity:
        phonetic_similarity = {'Medium': 1.0}
    if not orthographic_similarity:
        orthographic_similarity = {'Medium': 1.0}
    
    # Calculate required counts for each tier
    phonetic_counts = {}
    for tier in ['Light', 'Medium', 'Far']:
        phonetic_counts[tier] = int(non_rule_count * phonetic_similarity.get(tier, 0.0))
    
    orthographic_counts = {}
    for tier in ['Light', 'Medium', 'Far']:
        orthographic_counts[tier] = int(non_rule_count * orthographic_similarity.get(tier, 0.0))
    
    # Categorize candidates by both phonetic and orthographic similarity
    candidates_by_tiers = {
        'phonetic': {'Light': [], 'Medium': [], 'Far': []},
        'orthographic': {'Light': [], 'Medium': [], 'Far': []}
    }
    
    for candidate in candidate_pool:
        # Categorize by phonetic similarity
        phonetic_tier = get_metaphone_match_score(original_name, candidate)
        candidates_by_tiers['phonetic'][phonetic_tier].append(candidate)
        
        # Categorize by orthographic similarity
        orthographic_tier = get_levenshtein_tier(original_name, candidate)
        candidates_by_tiers['orthographic'][orthographic_tier].append(candidate)
    
    # CRITICAL: Filter candidates for uniqueness (validator checks combined_similarity > 0.99)
    # Pre-filter candidates to ensure they're not too similar to each other
    unique_candidates = []
    for candidate in candidate_pool:
        is_unique = True
        for unique_cand in unique_candidates:
            # Calculate combined similarity (same as validator: 0.7 phonetic + 0.3 orthographic)
            phonetic_sim = calculate_phonetic_similarity_score(unique_cand, candidate)
            orthographic_sim = calculate_orthographic_similarity_score(unique_cand, candidate)
            combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
            
            if combined_similarity > 0.99:  # Validator's uniqueness threshold
                is_unique = False
                break
        
        if is_unique:
            unique_candidates.append(candidate)
    
    # Use unique candidates pool
    candidate_pool = unique_candidates
    if not candidate_pool:
        # If all candidates are too similar, generate more diverse ones
        candidate_pool = generate_name_variations(original_name, limit=non_rule_count * 20)
        candidate_pool = [c for c in candidate_pool if c.lower() != original_name.lower()]
    
    # Select variations to match target distribution
    # CRITICAL: Use actual similarity scores to categorize, not heuristics
    selected = []
    used = set()
    
    # Calculate actual similarity scores for all candidates and categorize
    candidates_with_scores = []
    for candidate in candidate_pool:
        if candidate.lower() in used or candidate.lower() == original_name.lower():
            continue
        
        phonetic_score = calculate_phonetic_similarity_score(original_name, candidate)
        orthographic_score = calculate_orthographic_similarity_score(original_name, candidate)
        phonetic_tier = get_phonetic_tier_from_score(phonetic_score)
        orthographic_tier = get_orthographic_tier_from_score(orthographic_score)
        
        candidates_with_scores.append({
            'candidate': candidate,
            'phonetic_score': phonetic_score,
            'orthographic_score': orthographic_score,
            'phonetic_tier': phonetic_tier,
            'orthographic_tier': orthographic_tier
        })
    
    # Shuffle for randomness
    random.shuffle(candidates_with_scores)
    
    # Strategy 1: Prioritize candidates that satisfy BOTH phonetic AND orthographic requirements
    # Sort candidates by how well they match both requirements
    for cand_data in candidates_with_scores:
        if len(selected) >= non_rule_count:
            break
        
        candidate = cand_data['candidate']
        phonetic_tier = cand_data['phonetic_tier']
        orthographic_tier = cand_data['orthographic_tier']
        phonetic_score = cand_data['phonetic_score']
        orthographic_score = cand_data['orthographic_score']
        
        # Count how many we've already selected in each tier
        phonetic_selected_count = sum(1 for v in selected 
                                     if get_phonetic_tier_from_score(
                                         calculate_phonetic_similarity_score(original_name, v)
                                     ) == phonetic_tier)
        orthographic_selected_count = sum(1 for v in selected 
                                         if get_orthographic_tier_from_score(
                                             calculate_orthographic_similarity_score(original_name, v)
                                         ) == orthographic_tier)
        
        # Check if this candidate helps us meet our targets
        phonetic_needed = phonetic_counts.get(phonetic_tier, 0) > phonetic_selected_count
        orthographic_needed = orthographic_counts.get(orthographic_tier, 0) > orthographic_selected_count
        
        # CRITICAL: Check uniqueness against already selected variations
        # Validator checks combined_similarity > 0.99 for uniqueness penalty
        is_unique = True
        for selected_var in selected:
            phonetic_sim = calculate_phonetic_similarity_score(selected_var, candidate)
            orthographic_sim = calculate_orthographic_similarity_score(selected_var, candidate)
            combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
            
            if combined_similarity > 0.99:  # Validator's uniqueness threshold
                is_unique = False
                break
        
        # Priority: Select candidates that satisfy BOTH requirements first
        if is_unique:
            if phonetic_needed and orthographic_needed:
                # Perfect match - satisfies both requirements
                selected.append(candidate)
                used.add(candidate.lower())
            elif phonetic_needed or orthographic_needed:
                # Partial match - satisfies one requirement
                # Only add if we haven't met our targets yet
                selected.append(candidate)
                used.add(candidate.lower())
    
    # Strategy 2: Fill remaining slots prioritizing candidates that meet individual requirements
    if len(selected) < non_rule_count:
        remaining = non_rule_count - len(selected)
        for cand_data in candidates_with_scores:
            if len(selected) >= non_rule_count:
                break
            
            candidate = cand_data['candidate']
            if candidate.lower() in used:
                continue
            
            # Check uniqueness
            is_unique = True
            for selected_var in selected:
                phonetic_sim = calculate_phonetic_similarity_score(selected_var, candidate)
                orthographic_sim = calculate_orthographic_similarity_score(selected_var, candidate)
                combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
                
                if combined_similarity > 0.99:
                    is_unique = False
                    break
            
            if is_unique:
                selected.append(candidate)
                used.add(candidate.lower())
    
    # Strategy 3: Generate more candidates if still needed
    if len(selected) < non_rule_count:
        remaining = non_rule_count - len(selected)
        # Generate many more candidates to ensure diversity
        extra_candidates = generate_name_variations(original_name, limit=remaining * 20)
        extra_candidates = [c for c in extra_candidates if c.lower() != original_name.lower() and c.lower() not in used]
        
        # Filter for uniqueness and categorize
        for candidate in extra_candidates:
            if len(selected) >= non_rule_count:
                break
            
            # Check uniqueness
            is_unique = True
            for selected_var in selected:
                phonetic_sim = calculate_phonetic_similarity_score(selected_var, candidate)
                orthographic_sim = calculate_orthographic_similarity_score(selected_var, candidate)
                combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
                
                if combined_similarity > 0.99:
                    is_unique = False
                    break
            
            if is_unique:
                selected.append(candidate)
                used.add(candidate.lower())
                if len(selected) >= non_rule_count:
                    break
    
    # Debug: Log actual vs target distribution (optional, can be enabled for debugging)
    if len(selected) >= non_rule_count:
        # Calculate actual distribution for verification
        phonetic_dist = {'Light': 0, 'Medium': 0, 'Far': 0}
        orthographic_dist = {'Light': 0, 'Medium': 0, 'Far': 0}
        for var in selected:
            phonetic_tier = get_metaphone_match_score(original_name, var)
            orthographic_tier = get_levenshtein_tier(original_name, var)
            phonetic_dist[phonetic_tier] += 1
            orthographic_dist[orthographic_tier] += 1
        
        # Optional debug output (commented out for production)
        # print(f"   📊 Distribution - Phonetic: Light={phonetic_dist['Light']}/{phonetic_counts['Light']}, Medium={phonetic_dist['Medium']}/{phonetic_counts['Medium']}, Far={phonetic_dist['Far']}/{phonetic_counts['Far']}")
        # print(f"   📊 Distribution - Orthographic: Light={orthographic_dist['Light']}/{orthographic_counts['Light']}, Medium={orthographic_dist['Medium']}/{orthographic_counts['Medium']}, Far={orthographic_dist['Far']}/{orthographic_counts['Far']}")
    
    return selected[:non_rule_count]

# ============================================================================
# Main Generation Function
# ============================================================================

def generate_name_variations_clean(original_name: str, variation_count: int, 
                                   rule_percentage: float, rules: List[str],
                                   phonetic_similarity: Dict[str, float] = None,
                                   orthographic_similarity: Dict[str, float] = None) -> List[str]:
    """
    Generate name variations - rule-based and non-rule-based with tiered similarity targeting.
    
    Args:
        original_name: The original name to generate variations for
        variation_count: Total number of variations needed
        rule_percentage: Percentage of variations that should be rule-based
        rules: List of rule names to apply
        phonetic_similarity: Dict with Light/Medium/Far percentages (e.g., {'Light': 0.1, 'Medium': 0.3, 'Far': 0.6})
        orthographic_similarity: Dict with Light/Medium/Far percentages (e.g., {'Light': 0.7, 'Medium': 0.3})
    """
    # CRITICAL: Ensure exact rule percentage matching (validator checks this strictly)
    # Round to nearest integer for better accuracy
    rule_based_count = round(variation_count * rule_percentage)
    # Ensure we don't exceed total count
    rule_based_count = min(rule_based_count, variation_count)
    non_rule_count = variation_count - rule_based_count
    
    # Ensure non_rule_count is non-negative
    if non_rule_count < 0:
        non_rule_count = 0
        rule_based_count = variation_count
    
    variations = []
    used_variations = set()
    
    # Detect script type
    script = detect_script(original_name)
    is_non_latin = (script != 'latin')
    
    # Generate rule-based variations
    print(f"   🔧 Rule-based: {rule_based_count}")
    rule_attempts = {}
    for i in range(rule_based_count):
        if rules:
            rule = random.choice(rules)
            
            # Try applying the rule multiple times to get unique variations
            attempts = 0
            var = None
            while attempts < 20:  # Try up to 20 times to get a unique variation
                var = apply_rule_to_name(original_name, rule)
                
                # If we got a unique variation, use it
                if var.lower() not in used_variations and var != original_name:
                    break
                
                # If this rule always produces the same result, try a different rule
                if rule not in rule_attempts:
                    rule_attempts[rule] = 0
                rule_attempts[rule] += 1
                
                # If we've tried this rule too many times, pick a different one
                if rule_attempts[rule] > 5:
                    other_rules = [r for r in rules if r != rule]
                    if other_rules:
                        rule = random.choice(other_rules)
                        rule_attempts[rule] = 0
                    else:
                        # Only one rule available - break and try fallback strategies
                        break
                
                attempts += 1
            
            # Only add if we got a valid unique variation (NEVER add numeric suffixes)
            # CRITICAL: Check uniqueness using validator's combined_similarity threshold
            if var and var.lower() not in used_variations and var != original_name:
                # Check uniqueness against all existing variations (validator's threshold: > 0.99)
                is_unique = True
                for existing_var in variations:
                    phonetic_sim = calculate_phonetic_similarity_score(existing_var, var)
                    orthographic_sim = calculate_orthographic_similarity_score(existing_var, var)
                    combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
                    
                    if combined_similarity > 0.99:  # Validator's uniqueness threshold
                        is_unique = False
                        break
                
                if is_unique:
                    variations.append(var)
                    used_variations.add(var.lower())
            elif var and var == original_name and attempts < 20:
                # If rule didn't change the name, try a different rule
                for alt_rule in rules:
                    if alt_rule != rule:
                        var = apply_rule_to_name(original_name, alt_rule)
                        if var.lower() not in used_variations and var != original_name:
                            # Check uniqueness
                            is_unique = True
                            for existing_var in variations:
                                phonetic_sim = calculate_phonetic_similarity_score(existing_var, var)
                                orthographic_sim = calculate_orthographic_similarity_score(existing_var, var)
                                combined_similarity = phonetic_sim * 0.7 + orthographic_sim * 0.3
                                
                                if combined_similarity > 0.99:
                                    is_unique = False
                                    break
                            
                            if is_unique:
                                variations.append(var)
                                used_variations.add(var.lower())
                                break
    
    # Generate non-rule variations using tiered similarity targeting
    print(f"   🔬 Non-rule: {non_rule_count} (using tiered similarity targeting)")
    if non_rule_count > 0:
        # For non-Latin scripts, skip tiered approach and go straight to script-specific variations
        if is_non_latin:
            print(f"   🌍 Detected {script} script - using script-specific variations")
            # Request 3x more variations to ensure we have enough unique ones
            non_latin_vars = generate_non_latin_variations(original_name, script, non_rule_count * 3)
            
            for var in non_latin_vars:
                if len(variations) >= variation_count:
                    break
                if var.lower() not in used_variations:
                    variations.append(var)
                    used_variations.add(var.lower())
        else:
            # For Latin scripts, use tiered similarity targeting with jellyfish
            if JELLYFISH_AVAILABLE and (phonetic_similarity or orthographic_similarity):
                # Use tiered generation to match target distribution
                non_rule_vars = generate_tiered_name_variations(
                    original_name,
                    non_rule_count,
                    phonetic_similarity,
                    orthographic_similarity
                )
            else:
                # Fallback: use simple name_variations.py if jellyfish not available
                non_rule_vars = generate_name_variations(original_name, limit=non_rule_count * 3)
            
            for var in non_rule_vars:
                if len(variations) >= variation_count:
                    break
                if var.lower() not in used_variations:
                    variations.append(var)
                    used_variations.add(var.lower())
    
    # Final fallback - only if we still don't have enough
    # NEVER use numeric suffixes - use character-level transformations instead
    if len(variations) < variation_count:
        remaining = variation_count - len(variations)
        if is_non_latin:
            # For non-Latin, ALWAYS use script-specific variations - NEVER numeric suffixes
            print(f"   🌍 Generating {remaining} more {script} script variations (no numeric suffixes)")
            
            # Generate many more variations to ensure we have enough
            non_latin_vars = generate_non_latin_variations(original_name, script, remaining * 5)
            
            for var in non_latin_vars:
                if len(variations) >= variation_count:
                    break
                if var.lower() not in used_variations:
                    variations.append(var)
                    used_variations.add(var.lower())
        
        # For BOTH Latin and non-Latin: create character-level variations manually
        # This ensures we never fall back to numeric suffixes
        if len(variations) < variation_count:
            remaining = variation_count - len(variations)
            parts = original_name.split()
            attempts = 0
            max_attempts = remaining * 10  # Try many times to get unique variations
            
            while len(variations) < variation_count and attempts < max_attempts:
                attempts += 1
                
                if len(parts) >= 2:
                    # Try different part orders and combinations
                    strategy = attempts % 6
                    if strategy == 0:
                        var = " ".join(parts[::-1])  # Reverse order
                    elif strategy == 1:
                        var = "".join(parts)  # Merge parts
                    elif strategy == 2:
                        var = parts[-1] + " " + " ".join(parts[:-1])  # Last name first
                    elif strategy == 3:
                        var = " ".join([parts[1]] + [parts[0]] + parts[2:]) if len(parts) > 2 else " ".join(parts[::-1])  # Swap first two
                    elif strategy == 4:
                        # Try applying character transformations to individual parts
                        modified_parts = list(parts)
                        part_idx = attempts % len(modified_parts)
                        word = modified_parts[part_idx]
                        if len(word) > 1:
                            # Try removing a character
                            char_idx = (attempts // len(modified_parts)) % (len(word) - 1) + 1
                            modified_parts[part_idx] = word[:char_idx] + word[char_idx+1:]
                            var = " ".join(modified_parts)
                        else:
                            var = None
                    else:
                        # Try merging with different separators
                        separators = ['', '-', '_', '.']
                        sep = separators[attempts % len(separators)]
                        var = sep.join(parts)
                elif len(parts) == 1 and len(parts[0]) > 1:
                    # For single word, try various character-level transformations
                    word = parts[0]
                    word_len = len(word)
                    strategy = attempts % 5
                    
                    if strategy == 0:
                        # Remove a character from different positions
                        idx = (attempts // 5) % (word_len - 1) + 1
                        var = word[:idx] + word[idx+1:]
                    elif strategy == 1:
                        # Swap adjacent characters
                        idx = (attempts // 5) % (word_len - 1)
                        chars = list(word)
                        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
                        var = ''.join(chars)
                    elif strategy == 2:
                        # Duplicate a character
                        idx = (attempts // 5) % word_len
                        var = word[:idx+1] + word[idx:]
                    elif strategy == 3:
                        # Capitalize different positions
                        var = word[:1].upper() + word[1:].lower() if word[0].islower() else word
                    else:
                        # Try vowel substitutions (common misspellings)
                        vowels = 'aeiou'
                        for i, char in enumerate(word.lower()):
                            if char in vowels:
                                # Replace with a different vowel
                                new_vowel = vowels[(vowels.index(char) + (attempts // 5) % len(vowels)) % len(vowels)]
                                var = word[:i] + new_vowel + word[i+1:]
                                break
                        else:
                            var = word
                else:
                    continue
                
                # Only add if valid and unique
                if var and var.lower() not in used_variations and var != original_name:
                    variations.append(var)
                    used_variations.add(var.lower())
    
    return variations[:variation_count]

# ============================================================================
# Main Function
# ============================================================================

def generate_variations(synapse: IdentitySynapse) -> Dict:
    """
    Generate variations for all identities.
    Returns different structure for UAV seed vs normal seeds.
    """
    requirements = parse_query_template(synapse.query_template)
    
    print("=" * 80)
    print("CLEAN VARIATION GENERATOR - NO VALIDATION, NO SCORING")
    print("=" * 80)
    print(f"\n📋 Requirements:")
    print(f"   Variation count: {requirements['variation_count']}")
    print(f"   Rule percentage: {requirements['rule_percentage']*100:.0f}%")
    print(f"   Rules: {requirements['rules']}")
    if requirements.get('phonetic_similarity'):
        print(f"   🎵 Phonetic Similarity: {requirements['phonetic_similarity']}")
    if requirements.get('orthographic_similarity'):
        print(f"   📝 Orthographic Similarity: {requirements['orthographic_similarity']}")
    if requirements['uav_seed_name']:
        print(f"   🎯 UAV Seed: {requirements['uav_seed_name']}")
    print()
    
    all_variations = {}
    uav_seed_name = requirements['uav_seed_name']
    
    # CRITICAL: Ensure we process ALL identities from seed (no missing names)
    # Validator checks: missing_names = set(seed_names) - set(variations.keys())
    seed_names = [identity[0] for identity in synapse.identity if len(identity) > 0]
    
    for identity in synapse.identity:
        name = identity[0] if len(identity) > 0 else "Unknown"
        dob = identity[1] if len(identity) > 1 else "1990-01-01"
        address = identity[2] if len(identity) > 2 else "Unknown"
        
        print(f"🔄 Processing: {name}")
        is_uav_seed = (uav_seed_name and name.lower() == uav_seed_name.lower())
        
        if is_uav_seed:
            print(f"   🎯 This is the UAV seed - will include UAV data")
        
        # Generate variations with tiered similarity targeting
        name_vars = generate_name_variations_clean(
            original_name=name,
            variation_count=requirements['variation_count'],
            rule_percentage=requirements['rule_percentage'],
            rules=requirements['rules'],
            phonetic_similarity=requirements.get('phonetic_similarity'),
            orthographic_similarity=requirements.get('orthographic_similarity')
        )
        
        dob_vars = generate_dob_variations(dob, requirements['variation_count'])
        address_vars = generate_address_variations(address, requirements['variation_count'])
        
        # CRITICAL: Ensure we have EXACTLY the requested count
        # Validator requires exact count match for completeness multiplier
        variation_count = requirements['variation_count']
        
        # Ensure all arrays have at least the required count
        while len(name_vars) < variation_count:
            # Add more variations if needed
            name_vars.append(name)
        while len(dob_vars) < variation_count:
            dob_vars.append(dob)
        while len(address_vars) < variation_count:
            address_vars.append(address)
        
        # Trim to exact count
        name_vars = name_vars[:variation_count]
        dob_vars = dob_vars[:variation_count]
        address_vars = address_vars[:variation_count]
        
        # Combine into [name, dob, address] format
        # CRITICAL: Ensure no duplicates - validator penalizes duplicates
        combined = []
        seen_combinations = set()
        
        for i in range(variation_count):
            # Create unique combination by checking for duplicates
            name_var = name_vars[i]
            dob_var = dob_vars[i]
            addr_var = address_vars[i]
            
            # Normalize for duplicate detection (same as validator)
            combo_key = (
                name_var.lower().strip() if name_var else "",
                dob_var.strip() if dob_var else "",
                addr_var.lower().strip() if addr_var else ""
            )
            
            # If duplicate, modify slightly to make unique
            attempt = 0
            while combo_key in seen_combinations and attempt < 100:
                # Try next variation in arrays
                idx = (i + attempt) % variation_count
                name_var = name_vars[idx] if idx < len(name_vars) else name
                dob_var = dob_vars[idx] if idx < len(dob_vars) else dob
                addr_var = address_vars[idx] if idx < len(address_vars) else address
                combo_key = (
                    name_var.lower().strip() if name_var else "",
                    dob_var.strip() if dob_var else "",
                    addr_var.lower().strip() if addr_var else ""
                )
                attempt += 1
            
            # If still duplicate, create a unique one by modifying address slightly
            if combo_key in seen_combinations:
                # Add a unique suffix to address to make it unique
                addr_var = f"{addr_var} #UNQ{i}"
                combo_key = (
                    name_var.lower().strip() if name_var else "",
                    dob_var.strip() if dob_var else "",
                    addr_var.lower().strip() if addr_var else ""
                )
            
            seen_combinations.add(combo_key)
            combined.append([name_var, dob_var, addr_var])
        
        # CRITICAL: Ensure exact count (validator checks this strictly)
        combined = combined[:variation_count]
        
        # Phase 3: Return different structure for UAV seed
        if is_uav_seed:
            # Generate UAV address
            uav_data = generate_uav_address(address)
            print(f"   🎯 Generated UAV: {uav_data['address']} ({uav_data['label']})")
            print(f"      Coordinates: ({uav_data['latitude']}, {uav_data['longitude']})")
            
            # UAV seed structure: {name: {variations: [...], uav: {...}}}
            all_variations[name] = {
                'variations': combined,
                'uav': uav_data
            }
        else:
            # Normal structure: {name: [[name, dob, addr], ...]}
            all_variations[name] = combined
        
        print(f"   ✅ Generated {len(combined)} variations\n")
    
    # CRITICAL: Validate completeness before returning
    # 1. Check for missing names
    output_names = set(all_variations.keys())
    missing = set(seed_names) - output_names
    if missing:
        print(f"⚠️  WARNING: Missing names in output: {missing}")
        # Add missing names with empty variations (shouldn't happen, but safety check)
        for missing_name in missing:
            all_variations[missing_name] = []
    
    # 2. Check for extra names (names not in seed)
    extra = output_names - set(seed_names)
    if extra:
        print(f"⚠️  WARNING: Extra names in output (will be penalized): {extra}")
        # Remove extra names to avoid penalty
        for extra_name in list(extra):
            del all_variations[extra_name]
    
    # 3. Validate variation counts
    for name, variations in all_variations.items():
        if isinstance(variations, dict):
            # UAV structure
            var_list = variations.get('variations', [])
        else:
            var_list = variations
        
        expected_count = requirements['variation_count']
        actual_count = len(var_list)
        if actual_count != expected_count:
            print(f"⚠️  WARNING: {name}: {actual_count} variations (expected {expected_count})")
            # Ensure exact count
            if actual_count < expected_count:
                # Pad with last variation or default
                if var_list:
                    last_var = var_list[-1]
                    while len(var_list) < expected_count:
                        var_list.append(last_var.copy() if isinstance(last_var, list) else last_var)
                else:
                    # No variations - add default
                    default_identity = next((id for id in synapse.identity if id[0] == name), None)
                    if default_identity:
                        default_var = [
                            default_identity[0] if len(default_identity) > 0 else name,
                            default_identity[1] if len(default_identity) > 1 else "1990-01-01",
                            default_identity[2] if len(default_identity) > 2 else "Unknown"
                        ]
                        var_list = [default_var.copy() for _ in range(expected_count)]
                    else:
                        var_list = [[name, "1990-01-01", "Unknown"] for _ in range(expected_count)]
            else:
                # Trim to exact count
                var_list = var_list[:expected_count]
            
            # Update the variations
            if isinstance(variations, dict):
                variations['variations'] = var_list
                all_variations[name] = variations
            else:
                all_variations[name] = var_list
    
    return all_variations

# ============================================================================
# Entry Point
# ============================================================================

def main():
    import sys
    
    if len(sys.argv) < 2:
        input_file = "example_synapse.json"
    else:
        input_file = sys.argv[1]
    
    output_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    print(f"📂 Loading synapse from: {input_file}\n")
    
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    synapse = IdentitySynapse(
        identity=data['identity'],
        query_template=data['query_template'],
        timeout=data.get('timeout', 120.0)
    )
    
    variations = generate_variations(synapse)
    
    # Print results
    print("\n" + "=" * 80)
    print("RESULTS")
    print("=" * 80)
    
    for original_name, var_list in variations.items():
        print(f"\n📝 Variations for: {original_name}")
        for i, var in enumerate(var_list, 1):
            print(f"   {i}. {var[0]} | {var[1]} | {var[2]}")
    
    # Output JSON in EXACT format that miners send to validators
    # Format: {name: [[name_var, dob_var, address_var], ...]}
    output_data = variations
    
    # If output file specified, save it
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"\n💾 Saved to: {output_file}")
        print(f"   Format: Miner response format (synapse.variations)")
    else:
        # Print JSON to stdout (exactly like miner sends)
        print("\n" + "=" * 80)
        print("JSON OUTPUT (Miner Format)")
        print("=" * 80)
        print(json.dumps(output_data, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()

