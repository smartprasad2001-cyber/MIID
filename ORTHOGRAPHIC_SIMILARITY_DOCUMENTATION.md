# Orthographic Similarity: End-to-End Documentation

## Table of Contents
1. [Overview](#overview)
2. [Query Generation](#query-generation)
3. [Calculation Method](#calculation-method)
4. [Categorization](#categorization)
5. [Distribution Matching](#distribution-matching)
6. [Scoring Integration](#scoring-integration)
7. [Final Impact on Rewards](#final-impact-on-rewards)
8. [Examples](#examples)

---

## Overview

**Orthographic Similarity** measures how similar two strings are in terms of their **spelling** (character-by-character comparison), as opposed to **phonetic similarity** which measures how similar they **sound**.

In the MIID subnet, orthographic similarity is used alongside phonetic similarity to evaluate the quality of name variations generated by miners. This dual-metric approach ensures that variations are both:
- **Phonetically similar** (sound alike) - important for bypassing audio-based screening
- **Orthographically similar** (spell similarly) - important for bypassing text-based screening

**Key Difference:**
- **Phonetic**: "John" and "Jon" sound similar → High phonetic score
- **Orthographic**: "John" and "Jon" are spelled similarly → High orthographic score
- **Phonetic**: "John" and "Sean" sound similar → High phonetic score
- **Orthographic**: "John" and "Sean" are spelled differently → Lower orthographic score

---

## Query Generation

### Location
`MIID/validator/query_generator.py` - Lines 1406-1440

### Process

The validator generates queries with specific orthographic similarity requirements using a **weighted random selection** system:

```python
orthographic_configs_with_weights = [
    # Balanced distribution - high weight for balanced testing
    ({"Light": 0.3, "Medium": 0.4, "Far": 0.3}, 0.25),  # 25% probability
    # Focus on Medium similarity - most common real-world scenario
    ({"Light": 0.2, "Medium": 0.6, "Far": 0.2}, 0.20),  # 20% probability
    # Focus on Far similarity - important for edge cases
    ({"Light": 0.1, "Medium": 0.3, "Far": 0.6}, 0.15),  # 15% probability
    # Light-Medium mix - moderate weight
    ({"Light": 0.5, "Medium": 0.5}, 0.12),  # 12% probability
    # Medium-Far mix - moderate weight
    ({"Light": 0.1, "Medium": 0.5, "Far": 0.4}, 0.10),  # 10% probability
    # Only Medium similarity - common case
    ({"Medium": 1.0}, 0.08),  # 8% probability
    # High Light but not 100% - reduced frequency
    ({"Light": 0.7, "Medium": 0.3}, 0.05),  # 5% probability
    # Only Far similarity - edge case
    ({"Far": 1.0}, 0.03),  # 3% probability
    # Only Light similarity - reduced frequency
    ({"Light": 1.0}, 0.02),  # 2% probability
]
```

### Selection Mechanism

```python
def weighted_random_choice(configs_with_weights):
    configs, weights = zip(*configs_with_weights)
    return random.choices(configs, weights=weights, k=1)[0]

orthographic_config = weighted_random_choice(orthographic_configs_with_weights)
```

### Query Parameters

The selected configuration is passed to miners as part of the query, specifying:
- **Percentage of variations** that should fall into each category (Light, Medium, Far)
- **Example**: `{"Light": 0.2, "Medium": 0.6, "Far": 0.2}` means:
  - 20% of variations should be Light (high orthographic similarity)
  - 60% of variations should be Medium (moderate orthographic similarity)
  - 20% of variations should be Far (low orthographic similarity)

### Query Flow

```
QueryGenerator.build_queries()
    ↓
Select orthographic_config using weighted_random_choice()
    ↓
Include in query template sent to miners
    ↓
Miners generate variations based on requirements
    ↓
Validator receives responses
    ↓
Pass orthographic_similarity dict to get_name_variation_rewards()
```

---

## Calculation Method

### Location
`MIID/validator/reward.py` - Lines 171-191

### Algorithm: Levenshtein Distance

Orthographic similarity is calculated using the **Levenshtein distance** (also known as edit distance):

```python
def calculate_orthographic_similarity(original_name: str, variation: str) -> float:
    """
    Calculate orthographic similarity between two strings using Levenshtein distance.
    
    Args:
        original_name: The original name
        variation: The variation to compare against
        
    Returns:
        Orthographic similarity score between 0 and 1
    """
    try:
        # Use Levenshtein distance to compare
        distance = Levenshtein.distance(original_name, variation)
        max_len = max(len(original_name), len(variation))
        
        # Calculate orthographic similarity score (0-1)
        return 1.0 - (distance / max_len)
    except Exception as e:
        bt.logging.warning(f"Error calculating orthographic score: {str(e)}")
        return 0.0
```

### Formula

```
orthographic_similarity = 1.0 - (Levenshtein_distance / max_length)
```

Where:
- **Levenshtein_distance**: Minimum number of single-character edits (insertions, deletions, substitutions) needed to transform one string into another
- **max_length**: Maximum length between the two strings

### Examples

| Original | Variation | Levenshtein Distance | Max Length | Orthographic Score |
|----------|-----------|---------------------|------------|-------------------|
| "John" | "John" | 0 | 4 | 1.0 (100%) |
| "John" | "Jon" | 1 | 4 | 0.75 (75%) |
| "John" | "Jhon" | 1 | 4 | 0.75 (75%) |
| "John" | "Johny" | 1 | 5 | 0.80 (80%) |
| "John" | "Johnny" | 2 | 6 | 0.67 (67%) |
| "John" | "Sean" | 4 | 4 | 0.0 (0%) |
| "Smith" | "Smyth" | 1 | 5 | 0.80 (80%) |
| "Smith" | "Smithe" | 1 | 6 | 0.83 (83%) |
| "Smith" | "Smythe" | 2 | 6 | 0.67 (67%) |

### Characteristics

1. **Symmetric**: `calculate_orthographic_similarity("A", "B") == calculate_orthographic_similarity("B", "A")`
2. **Range**: Always returns a value between 0.0 and 1.0
3. **Perfect Match**: Returns 1.0 when strings are identical
4. **No Match**: Returns 0.0 when strings have no common characters (or when distance equals max length)

---

## Categorization

### Location
`MIID/validator/reward.py` - Lines 773-777 and 2263-2267

### Boundaries

Each variation's orthographic score is categorized into one of three levels:

```python
orthographic_boundaries = {
    "Light": (0.70, 1.00),   # High similarity range (70-100%)
    "Medium": (0.50, 0.69),  # Moderate similarity range (50-69%)
    "Far": (0.20, 0.49)      # Low similarity range (20-49%)
}
```

### Categorization Logic

For each variation:
1. Calculate orthographic similarity score using Levenshtein distance
2. Check which boundary range the score falls into:
   - **Light**: Score ≥ 0.70
   - **Medium**: 0.50 ≤ Score < 0.70
   - **Far**: 0.20 ≤ Score < 0.50
   - **Below threshold**: Score < 0.20 (treated as unmatched, penalized)

### Example Categorization

For original name "John Smith":

| Variation | Orthographic Score | Category |
|-----------|-------------------|----------|
| "John Smith" | 1.00 | Light |
| "Jon Smith" | 0.75 | Light |
| "Jhon Smith" | 0.75 | Light |
| "John Smyth" | 0.80 | Light |
| "John Smythe" | 0.67 | Medium |
| "Johny Smith" | 0.80 | Light |
| "Johnny Smith" | 0.67 | Medium |
| "Jon Smyth" | 0.75 | Light |
| "Jhon Smythe" | 0.67 | Medium |
| "Sean Smith" | 0.50 | Medium |
| "John Smth" | 0.60 | Medium |
| "Jhn Smit" | 0.50 | Medium |
| "John Smi" | 0.50 | Medium |
| "J Smith" | 0.40 | Far |
| "Xyz Abc" | 0.00 | Below threshold |

---

## Distribution Matching

### Location
`MIID/validator/reward.py` - Lines 882-920 and 925-927

### Process

After categorizing all variations, the system checks if the **actual distribution** matches the **expected distribution** specified in the query.

### Expected Distribution

From the query: `{"Light": 0.2, "Medium": 0.6, "Far": 0.2}`

If 15 variations are expected:
- **Light**: 15 × 0.2 = 3 variations
- **Medium**: 15 × 0.6 = 9 variations
- **Far**: 15 × 0.2 = 3 variations

### Distribution Quality Calculation

```python
def calculate_distribution_quality(scores, boundaries, targets):
    quality = 0.0
    total_matched = 0
    
    for level, (lower, upper) in boundaries.items():
        target_percentage = targets.get(level, 0.0)
        if target_percentage == 0.0:
            continue
            
        # Count scores in this range
        count = sum(1 for score in scores if lower <= score <= upper)
        target_count = int(target_percentage * len(scores))
        
        if target_count > 0:
            # Calculate match quality with diminishing returns
            match_ratio = count / target_count
            if match_ratio <= 1.0:
                match_quality = match_ratio  # Linear up to target
            else:    
                match_quality = 1.0 - math.exp(-(match_ratio - 1.0))  # Diminishing returns after target
            quality += target_percentage * match_quality
            total_matched += count
    
    # Penalize unmatched variations
    unmatched = len(scores) - total_matched
    if unmatched > 0:
        penalty = 0.1 * (unmatched / len(scores))
        quality = max(0.0, quality - penalty)
    
    return quality
```

### Match Quality Formula

For each category (Light, Medium, Far):

1. **Count actual variations** in that category
2. **Calculate target count**: `target_count = target_percentage × total_variations`
3. **Calculate match ratio**: `match_ratio = actual_count / target_count`
4. **Calculate match quality**:
   - If `match_ratio ≤ 1.0`: `match_quality = match_ratio` (linear, up to 100%)
   - If `match_ratio > 1.0`: `match_quality = 1.0 - exp(-(match_ratio - 1.0))` (diminishing returns)
5. **Weighted contribution**: `quality += target_percentage × match_quality`

### Penalty for Unmatched Variations

Variations that fall **below the Far threshold** (< 0.20) are penalized:
- **Penalty**: `0.1 × (unmatched_count / total_variations)`
- This reduces the overall quality score

### Example Distribution Matching

**Query Requirements**: `{"Light": 0.2, "Medium": 0.6, "Far": 0.2}` for 15 variations

**Expected Distribution**:
- Light: 3 variations
- Medium: 9 variations
- Far: 3 variations

**Actual Distribution** (example):
- Light: 4 variations (expected 3)
- Medium: 8 variations (expected 9)
- Far: 2 variations (expected 3)
- Below threshold: 1 variation (penalized)

**Calculation**:
1. **Light**: match_ratio = 4/3 = 1.33
   - match_quality = 1.0 - exp(-(1.33 - 1.0)) = 1.0 - exp(-0.33) ≈ 0.72
   - contribution = 0.2 × 0.72 = 0.144

2. **Medium**: match_ratio = 8/9 = 0.89
   - match_quality = 0.89 (linear)
   - contribution = 0.6 × 0.89 = 0.534

3. **Far**: match_ratio = 2/3 = 0.67
   - match_quality = 0.67 (linear)
   - contribution = 0.2 × 0.67 = 0.134

4. **Unmatched penalty**: 1/15 = 0.067
   - penalty = 0.1 × 0.067 = 0.0067

**Final Quality**: 0.144 + 0.534 + 0.134 - 0.0067 = **0.8053**

### Key Insights

1. **Exceeding target counts** gives diminishing returns (not penalized, but not rewarded linearly)
2. **Missing target counts** reduces quality linearly
3. **Unmatched variations** (below 0.20) are always penalized
4. **Perfect match** (exactly meeting targets) gives 100% quality for that category

---

## Scoring Integration

### Location
`MIID/validator/reward.py` - Lines 860-930

### Process Flow

For each name part (first name and last name separately):

1. **Calculate Individual Scores**:
   ```python
   for variation in unique_variations:
       p_score = calculate_phonetic_similarity(original_part, variation)
       o_score = calculate_orthographic_similarity(original_part, variation)
       phonetic_scores.append(p_score)
       orthographic_scores.append(o_score)
   ```

2. **Calculate Distribution Quality**:
   ```python
   phonetic_quality = calculate_distribution_quality(
       phonetic_scores, phonetic_boundaries, phonetic_similarity
   )
   orthographic_quality = calculate_distribution_quality(
       orthographic_scores, orthographic_boundaries, orthographic_similarity
   )
   ```

3. **Combine into Similarity Score**:
   ```python
   similarity_score = (phonetic_quality + orthographic_quality) / 2  # Average
   ```

### Combined Similarity Score

The orthographic quality and phonetic quality are **averaged** to create a combined similarity score:

```
similarity_score = (phonetic_quality + orthographic_quality) / 2
```

This means:
- Both metrics have **equal weight** (50% each)
- A miner must perform well on **both** metrics to get a high similarity score
- Poor performance on either metric will reduce the combined score

### Minimum Similarity Threshold

```python
min_similarity_threshold = 0.2
if similarity_score < min_similarity_threshold:
    # Severely reduce the score
    similarity_score = similarity_score * 0.1  # Reduce to 10% of original
```

If the combined similarity score is below 0.2, it's reduced to 10% of its value, effectively penalizing very poor similarity.

### Part Score Calculation

The similarity score is then combined with other factors:

```python
final_score = (
    similarity_score * MIID_REWARD_WEIGHTS["similarity_weight"] +
    count_score * MIID_REWARD_WEIGHTS["count_weight"] +
    uniqueness_score * MIID_REWARD_WEIGHTS["uniqueness_weight"] +
    length_score * MIID_REWARD_WEIGHTS["length_weight"]
)
```

Where:
- `similarity_weight = 0.60` (60% of part score)
- `count_weight = 0.15` (15% of part score)
- `uniqueness_weight = 0.10` (10% of part score)
- `length_weight = 0.15` (15% of part score)

### Full Name Score

First and last name scores are combined:

```python
first_name_score = calculate_part_score(first_name, first_variations, ...)
last_name_score = calculate_part_score(last_name, last_variations, ...)

full_name_score = (
    first_name_score * MIID_REWARD_WEIGHTS["first_name_weight"] +
    last_name_score * MIID_REWARD_WEIGHTS["last_name_weight"]
)
```

Where:
- `first_name_weight = 0.3` (30% of full name score)
- `last_name_weight = 0.7` (70% of full name score)

---

## Final Impact on Rewards

### Location
`MIID/validator/reward.py` - Lines 2212-3007

### Weight Hierarchy

The orthographic similarity flows through multiple layers of weighting:

```
1. Orthographic Quality (0-1)
   ↓
2. Combined Similarity Score = (Phonetic + Orthographic) / 2
   ↓
3. Part Score = Similarity × 0.60 + Count × 0.15 + Uniqueness × 0.10 + Length × 0.15
   ↓
4. Full Name Score = First × 0.3 + Last × 0.7
   ↓
5. Final Reward = Full Name Score + DOB Score + Address Score + Rule Compliance Score
```

### Impact Calculation

**Example**: A miner generates variations with perfect orthographic distribution:

1. **Orthographic Quality**: 1.0 (perfect match)
2. **Phonetic Quality**: 0.8 (good but not perfect)
3. **Combined Similarity**: (1.0 + 0.8) / 2 = 0.9
4. **Part Score** (assuming other factors are perfect):
   - Similarity: 0.9 × 0.60 = 0.54
   - Count: 1.0 × 0.15 = 0.15
   - Uniqueness: 1.0 × 0.10 = 0.10
   - Length: 1.0 × 0.15 = 0.15
   - **Part Score**: 0.94

5. **Full Name Score** (assuming first and last are similar):
   - First: 0.94 × 0.3 = 0.282
   - Last: 0.94 × 0.7 = 0.658
   - **Full Name Score**: 0.94

### Maximum Impact

If orthographic quality is **perfect (1.0)** and phonetic quality is also **perfect (1.0)**:
- Combined similarity: 1.0
- Similarity contribution to part score: 1.0 × 0.60 = **0.60** (60% of part score)

If orthographic quality is **poor (0.3)** and phonetic quality is **perfect (1.0)**:
- Combined similarity: (0.3 + 1.0) / 2 = 0.65
- Similarity contribution to part score: 0.65 × 0.60 = **0.39** (39% of part score)
- **Loss**: 0.21 points (35% reduction)

### Real-World Impact

Orthographic similarity can significantly impact rewards:

1. **High Impact Scenarios**:
   - When query requires specific orthographic distribution (e.g., 60% Medium)
   - When variations have poor spelling similarity (low Levenshtein scores)
   - When distribution doesn't match requirements

2. **Moderate Impact Scenarios**:
   - When query has balanced requirements
   - When variations are generally well-spelled

3. **Low Impact Scenarios**:
   - When other factors (count, uniqueness, length) are poor
   - When phonetic similarity is also poor

---

## Examples

### Example 1: Perfect Orthographic Distribution

**Query**: `{"Light": 0.2, "Medium": 0.6, "Far": 0.2}` for 15 variations

**Original Name**: "John Smith"

**Generated Variations** (with orthographic scores):
- "John Smith" (1.00) - Light
- "Jon Smith" (0.75) - Light
- "Jhon Smith" (0.75) - Light
- "John Smyth" (0.80) - Light
- "John Smythe" (0.67) - Medium
- "Johny Smith" (0.80) - Light
- "Johnny Smith" (0.67) - Medium
- "Jon Smyth" (0.75) - Light
- "Jhon Smythe" (0.67) - Medium
- "Sean Smith" (0.50) - Medium
- "John Smth" (0.60) - Medium
- "Jhn Smit" (0.50) - Medium
- "John Smi" (0.50) - Medium
- "J Smith" (0.40) - Far
- "Jhn Smth" (0.40) - Far

**Distribution**:
- Light: 5 variations (expected 3) → match_ratio = 1.67 → quality ≈ 0.49 → contribution = 0.098
- Medium: 7 variations (expected 9) → match_ratio = 0.78 → quality = 0.78 → contribution = 0.468
- Far: 3 variations (expected 3) → match_ratio = 1.0 → quality = 1.0 → contribution = 0.2

**Orthographic Quality**: 0.098 + 0.468 + 0.2 = **0.766**

### Example 2: Poor Orthographic Distribution

**Query**: `{"Light": 0.2, "Medium": 0.6, "Far": 0.2}` for 15 variations

**Original Name**: "John Smith"

**Generated Variations** (with orthographic scores):
- "John Smith" (1.00) - Light
- "Sean Smith" (0.50) - Medium
- "Shawn Smith" (0.50) - Medium
- "Shon Smith" (0.50) - Medium
- "Shane Smith" (0.50) - Medium
- "Xyz Abc" (0.00) - Below threshold
- "Qwerty Zxcv" (0.00) - Below threshold
- ... (all variations have low orthographic similarity)

**Distribution**:
- Light: 1 variation (expected 3) → match_ratio = 0.33 → quality = 0.33 → contribution = 0.066
- Medium: 4 variations (expected 9) → match_ratio = 0.44 → quality = 0.44 → contribution = 0.264
- Far: 0 variations (expected 3) → match_ratio = 0.0 → quality = 0.0 → contribution = 0.0
- Below threshold: 10 variations → penalty = 0.1 × (10/15) = 0.067

**Orthographic Quality**: 0.066 + 0.264 + 0.0 - 0.067 = **0.263**

### Example 3: Impact on Final Score

**Scenario**: Miner A vs Miner B

**Miner A**:
- Orthographic Quality: 0.9
- Phonetic Quality: 0.9
- Combined Similarity: 0.9
- Other factors: Perfect (1.0)
- **Part Score**: 0.9 × 0.60 + 1.0 × 0.15 + 1.0 × 0.10 + 1.0 × 0.15 = **0.94**

**Miner B**:
- Orthographic Quality: 0.5
- Phonetic Quality: 0.9
- Combined Similarity: 0.7
- Other factors: Perfect (1.0)
- **Part Score**: 0.7 × 0.60 + 1.0 × 0.15 + 1.0 × 0.10 + 1.0 × 0.15 = **0.82**

**Difference**: 0.94 - 0.82 = **0.12** (12.8% reduction in part score)

---

## Summary

1. **Query Generation**: Orthographic similarity requirements are randomly selected using weighted probabilities, with Medium similarity being most common (60% of variations).

2. **Calculation**: Uses Levenshtein distance to measure character-by-character spelling similarity, returning a score between 0.0 and 1.0.

3. **Categorization**: Variations are categorized into Light (0.70-1.00), Medium (0.50-0.69), or Far (0.20-0.49) based on their orthographic scores.

4. **Distribution Matching**: The actual distribution of variations is compared against the expected distribution, with quality calculated using match ratios and diminishing returns for exceeding targets.

5. **Scoring Integration**: Orthographic quality is averaged with phonetic quality to create a combined similarity score, which contributes 60% to the part score.

6. **Final Impact**: Orthographic similarity can significantly impact rewards, with perfect distribution contributing up to 60% of the part score, and poor distribution potentially reducing rewards by 35% or more.

---

## Key Takeaways

- **Orthographic similarity measures spelling similarity**, not sound similarity
- **Levenshtein distance** is the core algorithm (edit distance)
- **Distribution matching** is critical - miners must meet the exact percentages specified in the query
- **Equal weight** with phonetic similarity (50/50 split in combined score)
- **60% contribution** to part score through similarity weight
- **Penalties apply** for variations below 0.20 threshold and for unmatched variations
- **Diminishing returns** for exceeding target counts (not linearly rewarded)

---

*Documentation generated: 2025-01-29*
*Last updated: Based on MIID/validator/reward.py and MIID/validator/query_generator.py*

